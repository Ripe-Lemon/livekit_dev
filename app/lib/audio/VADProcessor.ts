'use client';

export interface VADResult {
    probability: number; // 0-1 èŒƒå›´ï¼Œè¯­éŸ³æ£€æµ‹æ¦‚ç‡
    isSpeaking: boolean; // æ˜¯å¦æ­£åœ¨è¯´è¯
    volume: number; // éŸ³é‡çº§åˆ« 0-1
}

export interface VADConfig {
    threshold: number; // è¯­éŸ³æ£€æµ‹é˜ˆå€¼ 0-1
    smoothingFactor: number; // å¹³æ»‘å› å­ 0-1
    minSpeechFrames: number; // æœ€å°è¯­éŸ³å¸§æ•°
    minSilenceFrames: number; // æœ€å°é™éŸ³å¸§æ•°
    analyzeWindow: number; // åˆ†æçª—å£å¤§å°(æ¯«ç§’)
}

const DEFAULT_VAD_CONFIG: VADConfig = {
    threshold: 0.3,
    smoothingFactor: 0.8,
    minSpeechFrames: 3,
    minSilenceFrames: 10,
    analyzeWindow: 30 // 30ms
};

export class VADProcessor {
    private audioContext: AudioContext | null = null;
    private analyserNode: AnalyserNode | null = null;
    private microphoneSource: MediaStreamAudioSourceNode | null = null;
    private isActive = false;
    private config: VADConfig;
    private dataArray: Uint8Array | null = null;
    private freqDataArray: Uint8Array | null = null;
    
    // VAD çŠ¶æ€
    private currentVolume = 0;
    private smoothedVolume = 0;
    private speechFrameCount = 0;
    private silenceFrameCount = 0;
    private isSpeaking = false;
    private lastAnalysisTime = 0;
    
    // é¢‘è°±åˆ†æç›¸å…³
    private speechFreqBands = {
        low: { min: 85, max: 255 },      // 85-255 Hz (åŸºé¢‘)
        mid: { min: 255, max: 2000 },    // 255-2000 Hz (ä¸»è¦è¯­éŸ³é¢‘æ®µ)
        high: { min: 2000, max: 4000 }   // 2000-4000 Hz (è¯­éŸ³æ¸…æ™°åº¦)
    };

    // å†å²æ•°æ®ç”¨äºæ›´æ™ºèƒ½çš„æ£€æµ‹
    private volumeHistory: number[] = [];
    private readonly historySize = 10;

    // å›è°ƒå‡½æ•°
    private onVADUpdate: ((result: VADResult) => void) | null = null;

    // æ·»åŠ è°ƒè¯•çŠ¶æ€
    private debugMode = process.env.NODE_ENV === 'development';
    private analysisCount = 0;
    private lastVolumeUpdate = 0;

    constructor(config: Partial<VADConfig> = {}) {
        this.config = { ...DEFAULT_VAD_CONFIG, ...config };
        this.initializeAudioContext();
    }

    private initializeAudioContext() {
        try {
            this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
            console.log('ğŸ¤ VAD AudioContext å·²åˆå§‹åŒ–');
        } catch (error) {
            console.error('âŒ VAD AudioContext åˆå§‹åŒ–å¤±è´¥:', error);
        }
    }

    async connectToMicrophone(stream: MediaStream): Promise<void> {
        if (!this.audioContext) {
            throw new Error('AudioContext æœªåˆå§‹åŒ–');
        }

        try {
            // éªŒè¯è¾“å…¥æµ
            const audioTracks = stream.getAudioTracks();
            if (audioTracks.length === 0) {
                throw new Error('è¾“å…¥æµä¸­æ²¡æœ‰éŸ³é¢‘è½¨é“');
            }

            const audioTrack = audioTracks[0];
            console.log('ğŸ” VADè¿æ¥éŸ³é¢‘è½¨é“:', {
                label: audioTrack.label,
                readyState: audioTrack.readyState,
                enabled: audioTrack.enabled,
                muted: audioTrack.muted,
                settings: audioTrack.getSettings()
            });

            // æ¢å¤ AudioContextï¼ˆå¦‚æœè¢«æš‚åœï¼‰
            if (this.audioContext.state === 'suspended') {
                await this.audioContext.resume();
                console.log('ğŸ”„ AudioContext å·²æ¢å¤');
            }

            // åˆ›å»ºéŸ³é¢‘åˆ†æèŠ‚ç‚¹
            this.analyserNode = this.audioContext.createAnalyser();
            this.analyserNode.fftSize = 2048;
            this.analyserNode.smoothingTimeConstant = 0.8;
            this.analyserNode.minDecibels = -90;
            this.analyserNode.maxDecibels = -10;

            // è¿æ¥éº¦å…‹é£è¾“å…¥
            this.microphoneSource = this.audioContext.createMediaStreamSource(stream);
            this.microphoneSource.connect(this.analyserNode);

            // åˆå§‹åŒ–æ•°æ®æ•°ç»„
            const bufferLength = this.analyserNode.frequencyBinCount;
            this.dataArray = new Uint8Array(bufferLength);
            this.freqDataArray = new Uint8Array(bufferLength);

            console.log('âœ… VAD å·²è¿æ¥åˆ°éº¦å…‹é£', {
                fftSize: this.analyserNode.fftSize,
                bufferLength,
                sampleRate: this.audioContext.sampleRate,
                audioContextState: this.audioContext.state
            });

            // é‡ç½®è®¡æ•°å™¨
            this.analysisCount = 0;
            this.lastVolumeUpdate = Date.now();

            this.startAnalysis();
        } catch (error) {
            console.error('âŒ VAD è¿æ¥éº¦å…‹é£å¤±è´¥:', error);
            throw error;
        }
    }

    private startAnalysis() {
        if (this.isActive) return;
        
        this.isActive = true;
        console.log('ğŸ” VAD åˆ†æå·²å¼€å§‹');
        
        const analyze = () => {
            if (!this.isActive || !this.analyserNode || !this.dataArray || !this.freqDataArray) {
                return;
            }

            const now = Date.now();
            if (now - this.lastAnalysisTime < this.config.analyzeWindow) {
                requestAnimationFrame(analyze);
                return;
            }
            this.lastAnalysisTime = now;

            try {
                // è·å–éŸ³é¢‘æ•°æ®
                this.analyserNode.getByteTimeDomainData(this.dataArray);
                this.analyserNode.getByteFrequencyData(this.freqDataArray);

                // éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                const hasData = this.dataArray.some(value => value !== 128) || 
                               this.freqDataArray.some(value => value > 0);

                if (!hasData) {
                    if (this.debugMode && this.analysisCount % 100 === 0) {
                        console.warn('âš ï¸ VADæœªæ£€æµ‹åˆ°éŸ³é¢‘æ•°æ®ï¼Œæ£€æŸ¥éº¦å…‹é£è¿æ¥');
                    }
                } else {
                    // åˆ†æéŸ³é¢‘
                    const result = this.analyzeAudio();
                    
                    // è§¦å‘å›è°ƒ
                    if (this.onVADUpdate) {
                        this.onVADUpdate(result);
                    }

                    // è°ƒè¯•è¾“å‡ºï¼ˆé™ä½é¢‘ç‡ï¼‰
                    if (this.debugMode && this.analysisCount % 50 === 0) {
                        console.log('ğŸ” VADæ•°æ®é‡‡æ ·:', {
                            analysisCount: this.analysisCount,
                            volume: result.volume.toFixed(3),
                            probability: result.probability.toFixed(3),
                            isSpeaking: result.isSpeaking,
                            dataArraySample: Array.from(this.dataArray.slice(0, 10)),
                            freqArraySample: Array.from(this.freqDataArray.slice(0, 10)),
                            audioContextState: this.audioContext?.state
                        });
                    }
                }

                this.analysisCount++;
            } catch (error) {
                console.error('âŒ VADåˆ†æé”™è¯¯:', error);
            }

            requestAnimationFrame(analyze);
        };

        requestAnimationFrame(analyze);
    }

    private analyzeAudio(): VADResult {
        if (!this.dataArray || !this.freqDataArray) {
            return { probability: 0, isSpeaking: false, volume: 0 };
        }

        // 1. è®¡ç®—éŸ³é‡ (RMS)
        const volume = this.calculateVolume();
        this.currentVolume = volume;

        // 2. å¹³æ»‘éŸ³é‡
        this.smoothedVolume = this.smoothedVolume * this.config.smoothingFactor + 
                             volume * (1 - this.config.smoothingFactor);

        // 3. é¢‘è°±åˆ†æ
        const spectralFeatures = this.analyzeSpectrum();

        // 4. è¯­éŸ³æ¦‚ç‡è®¡ç®—
        const probability = this.calculateSpeechProbability(this.smoothedVolume, spectralFeatures);

        // 5. çŠ¶æ€æœºé€»è¾‘
        const wasSpeaking = this.isSpeaking;
        this.updateSpeechState(probability);

        // 6. æ›´æ–°å†å²æ•°æ®
        this.updateHistory(volume);

        // 7. è°ƒè¯•æ—¥å¿—ï¼ˆé™åˆ¶é¢‘ç‡ï¼‰
        const now = Date.now();
        if (this.debugMode && volume > 0.01 && now - this.lastVolumeUpdate > 1000) {
            console.log('ğŸ” VAD æ´»è·ƒåˆ†æ:', {
                volume: volume.toFixed(3),
                smoothed: this.smoothedVolume.toFixed(3),
                probability: probability.toFixed(3),
                isSpeaking: this.isSpeaking,
                spectral: {
                    speechEnergy: spectralFeatures.speechEnergy.toFixed(3),
                    totalEnergy: spectralFeatures.totalEnergy.toFixed(3),
                    centroid: spectralFeatures.spectralCentroid.toFixed(1)
                }
            });
            this.lastVolumeUpdate = now;
        }

        return {
            probability,
            isSpeaking: this.isSpeaking,
            volume: this.smoothedVolume
        };
    }

    private calculateVolume(): number {
        if (!this.dataArray) return 0;

        let sum = 0;
        let nonZeroCount = 0;
        
        for (let i = 0; i < this.dataArray.length; i++) {
            const amplitude = (this.dataArray[i] - 128) / 128;
            sum += amplitude * amplitude;
            if (this.dataArray[i] !== 128) nonZeroCount++;
        }
        
        // å¦‚æœæ‰€æœ‰æ•°æ®éƒ½æ˜¯128ï¼ˆé™éŸ³ï¼‰ï¼Œè¿”å›0
        if (nonZeroCount === 0) {
            return 0;
        }
        
        const rms = Math.sqrt(sum / this.dataArray.length);
        const volume = Math.min(rms * 3, 1); // å¢åŠ æ”¾å¤§å€æ•°ä»¥æé«˜æ•æ„Ÿåº¦
        
        return volume;
    }

    private analyzeSpectrum(): { speechEnergy: number; totalEnergy: number; spectralCentroid: number } {
        if (!this.freqDataArray || !this.audioContext) {
            return { speechEnergy: 0, totalEnergy: 0, spectralCentroid: 0 };
        }

        const nyquist = this.audioContext.sampleRate / 2;
        const binWidth = nyquist / this.freqDataArray.length;

        let totalEnergy = 0;
        let speechEnergy = 0;
        let weightedFreqSum = 0;
        let magnitudeSum = 0;

        for (let i = 0; i < this.freqDataArray.length; i++) {
            const frequency = i * binWidth;
            const magnitude = this.freqDataArray[i] / 255; // å½’ä¸€åŒ–åˆ°0-1
            
            totalEnergy += magnitude;
            magnitudeSum += magnitude;
            weightedFreqSum += frequency * magnitude;

            // è¯­éŸ³é¢‘æ®µèƒ½é‡ (85Hz - 4000Hz)
            if (frequency >= 85 && frequency <= 4000) {
                speechEnergy += magnitude;
            }
        }

        const spectralCentroid = magnitudeSum > 0 ? weightedFreqSum / magnitudeSum : 0;

        return {
            speechEnergy: speechEnergy / this.freqDataArray.length,
            totalEnergy: totalEnergy / this.freqDataArray.length,
            spectralCentroid
        };
    }

    private calculateSpeechProbability(volume: number, spectral: any): number {
        // å¤šç»´åº¦è¯­éŸ³æ¦‚ç‡è®¡ç®—
        
        // 1. éŸ³é‡æ¦‚ç‡ (Så‹æ›²çº¿)
        const volumeProb = 1 / (1 + Math.exp(-10 * (volume - this.config.threshold)));

        // 2. é¢‘è°±æ¦‚ç‡ (è¯­éŸ³é¢‘æ®µèƒ½é‡æ¯”ä¾‹)
        const spectralProb = spectral.totalEnergy > 0 ? 
            Math.min(spectral.speechEnergy / spectral.totalEnergy * 2, 1) : 0;

        // 3. é¢‘è°±é‡å¿ƒæ¦‚ç‡ (è¯­éŸ³é€šå¸¸åœ¨500-2000Hz)
        const centroidProb = spectral.spectralCentroid > 500 && spectral.spectralCentroid < 2000 ? 
            1 - Math.abs(spectral.spectralCentroid - 1250) / 1250 : 0;

        // 4. å†å²è¶‹åŠ¿æ¦‚ç‡
        const trendProb = this.calculateTrendProbability();

        // åŠ æƒç»„åˆ
        const probability = 
            volumeProb * 0.4 +      // éŸ³é‡æƒé‡æœ€é«˜
            spectralProb * 0.3 +    // é¢‘è°±èƒ½é‡æ¬¡ä¹‹
            centroidProb * 0.2 +    // é¢‘è°±é‡å¿ƒ
            trendProb * 0.1;        // å†å²è¶‹åŠ¿

        return Math.max(0, Math.min(1, probability));
    }

    private calculateTrendProbability(): number {
        if (this.volumeHistory.length < 3) return 0.5;

        const recent = this.volumeHistory.slice(-3);
        const avg = recent.reduce((a, b) => a + b, 0) / recent.length;
        const current = this.currentVolume;

        // å¦‚æœå½“å‰éŸ³é‡æ˜¾è‘—é«˜äºæœ€è¿‘å¹³å‡å€¼ï¼Œå¢åŠ è¯­éŸ³æ¦‚ç‡
        if (current > avg * 1.2) {
            return 0.8;
        } else if (current < avg * 0.8) {
            return 0.2;
        }
        return 0.5;
    }

    private updateSpeechState(probability: number) {
        if (probability > this.config.threshold) {
            this.speechFrameCount++;
            this.silenceFrameCount = 0;
            
            if (!this.isSpeaking && this.speechFrameCount >= this.config.minSpeechFrames) {
                this.isSpeaking = true;
                console.log('ğŸ—£ï¸ æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹');
            }
        } else {
            this.silenceFrameCount++;
            this.speechFrameCount = 0;
            
            if (this.isSpeaking && this.silenceFrameCount >= this.config.minSilenceFrames) {
                this.isSpeaking = false;
                console.log('ğŸ¤« æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ');
            }
        }
    }

    private updateHistory(volume: number) {
        this.volumeHistory.push(volume);
        if (this.volumeHistory.length > this.historySize) {
            this.volumeHistory.shift();
        }
    }

    updateConfig(newConfig: Partial<VADConfig>) {
        this.config = { ...this.config, ...newConfig };
        console.log('âš™ï¸ VAD é…ç½®å·²æ›´æ–°:', this.config);
    }

    setVADCallback(callback: (result: VADResult) => void) {
        this.onVADUpdate = callback;
    }

    stopAnalysis() {
        this.isActive = false;
        console.log('â¹ï¸ VAD åˆ†æå·²åœæ­¢');
    }

    disconnect() {
        this.stopAnalysis();
        
        if (this.microphoneSource) {
            this.microphoneSource.disconnect();
            this.microphoneSource = null;
        }
        
        if (this.analyserNode) {
            this.analyserNode.disconnect();
            this.analyserNode = null;
        }
        
        this.dataArray = null;
        this.freqDataArray = null;
        this.onVADUpdate = null;
        
        console.log('ğŸ”Œ VAD å·²æ–­å¼€è¿æ¥');
    }

    dispose() {
        this.disconnect();
        
        if (this.audioContext && this.audioContext.state !== 'closed') {
            this.audioContext.close();
            this.audioContext = null;
        }
        
        console.log('ğŸ—‘ï¸ VAD å·²é”€æ¯');
    }

    // æ·»åŠ å¼ºåˆ¶æµ‹è¯•æ–¹æ³•
    testAudioInput(): void {
        if (!this.analyserNode || !this.dataArray || !this.freqDataArray) {
            console.error('âŒ VADæœªåˆå§‹åŒ–ï¼Œæ— æ³•æµ‹è¯•');
            return;
        }

        console.log('ğŸ§ª å¼€å§‹VADéŸ³é¢‘è¾“å…¥æµ‹è¯•...');
        
        const testInterval = setInterval(() => {
            this.analyserNode!.getByteTimeDomainData(this.dataArray!);
            this.analyserNode!.getByteFrequencyData(this.freqDataArray!);
            
            const volume = this.calculateVolume();
            const hasTimeData = this.dataArray!.some(v => v !== 128);
            const hasFreqData = this.freqDataArray!.some(v => v > 0);
            
            console.log('ğŸ§ª æµ‹è¯•ç»“æœ:', {
                volume: volume.toFixed(4),
                hasTimeData,
                hasFreqData,
                timeDataSample: Array.from(this.dataArray!.slice(0, 5)),
                freqDataSample: Array.from(this.freqDataArray!.slice(0, 5)),
                audioContextState: this.audioContext?.state
            });
            
            if (volume > 0.01) {
                console.log('âœ… æ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥ï¼');
                clearInterval(testInterval);
            }
        }, 500);
        
        // 10ç§’ååœæ­¢æµ‹è¯•
        setTimeout(() => {
            clearInterval(testInterval);
            console.log('ğŸ§ª VADéŸ³é¢‘æµ‹è¯•ç»“æŸ');
        }, 10000);
    }

    // è·å–å½“å‰çŠ¶æ€ç”¨äºè°ƒè¯•
    getDebugInfo() {
        return {
            isActive: this.isActive,
            config: this.config,
            currentVolume: this.currentVolume,
            smoothedVolume: this.smoothedVolume,
            isSpeaking: this.isSpeaking,
            speechFrameCount: this.speechFrameCount,
            silenceFrameCount: this.silenceFrameCount,
            volumeHistory: [...this.volumeHistory],
            audioContextState: this.audioContext?.state,
            analysisCount: this.analysisCount,
            hasAnalyser: !!this.analyserNode,
            hasDataArrays: !!(this.dataArray && this.freqDataArray),
            dataArrayLength: this.dataArray?.length || 0,
            lastAnalysisTime: this.lastAnalysisTime
        };
    }
}