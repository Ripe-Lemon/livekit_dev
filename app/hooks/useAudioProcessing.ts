// app/hooks/useAudioProcessing.ts
'use client';

import { useState, useEffect, useCallback, useRef } from 'react';
import { useLocalParticipant, useRoomContext } from '@livekit/components-react';
import { Track, createLocalAudioTrack, LocalAudioTrack, AudioCaptureOptions } from 'livekit-client';

export interface AudioProcessingSettings {
    autoGainControl: boolean;
    noiseSuppression: boolean;
    echoCancellation: boolean;
    voiceIsolation: boolean;  // æ–°å¢ï¼šè¯­éŸ³éš”ç¦»
    microphoneThreshold: number; // 0-1 èŒƒå›´ï¼Œéº¦å…‹é£æ”¶éŸ³é—¨é™
    sampleRate: number;
    channelCount: number;
    latency: number;
}

export interface AudioProcessingControls {
    settings: AudioProcessingSettings;
    updateSetting: (key: keyof AudioProcessingSettings, value: boolean | number) => Promise<void>;
    isApplying: (key: keyof AudioProcessingSettings) => boolean;
    resetToDefaults: () => Promise<void>;
    isProcessingActive: boolean;
}

const DEFAULT_SETTINGS: AudioProcessingSettings = {
    autoGainControl: true,
    noiseSuppression: true,
    echoCancellation: false,
    voiceIsolation: false,
    microphoneThreshold: 0.3,
    sampleRate: 48000,
    channelCount: 1,
    latency: 0.01
};

// æœ¬åœ°å­˜å‚¨é”®
const STORAGE_KEY = 'livekit_audio_processing_settings';

export function useAudioProcessing(): AudioProcessingControls {
    const { localParticipant } = useLocalParticipant();
    const room = useRoomContext();
    
    // ä»æœ¬åœ°å­˜å‚¨åŠ è½½è®¾ç½®
    const loadSettings = useCallback((): AudioProcessingSettings => {
        if (typeof window === 'undefined') return DEFAULT_SETTINGS;
        
        try {
            const stored = localStorage.getItem(STORAGE_KEY);
            if (stored) {
                const parsed = JSON.parse(stored);
                return { ...DEFAULT_SETTINGS, ...parsed };
            }
        } catch (error) {
            console.warn('åŠ è½½éŸ³é¢‘å¤„ç†è®¾ç½®å¤±è´¥:', error);
        }
        return DEFAULT_SETTINGS;
    }, []);

    const [settings, setSettings] = useState<AudioProcessingSettings>(loadSettings);
    const [applyingSettings, setApplyingSettings] = useState<Set<string>>(new Set());
    const [isProcessingActive, setIsProcessingActive] = useState(false);
    
    // éŸ³é¢‘è½¨é“å¼•ç”¨
    const currentTrackRef = useRef<LocalAudioTrack | null>(null);
    const initializationRef = useRef<boolean>(false);

    // ä¿å­˜è®¾ç½®åˆ°æœ¬åœ°å­˜å‚¨
    const saveSettings = useCallback((newSettings: AudioProcessingSettings) => {
        if (typeof window !== 'undefined') {
            try {
                localStorage.setItem(STORAGE_KEY, JSON.stringify(newSettings));
            } catch (error) {
                console.warn('ä¿å­˜éŸ³é¢‘å¤„ç†è®¾ç½®å¤±è´¥:', error);
            }
        }
    }, []);

    // åˆ›å»ºéŸ³é¢‘è½¨é“é…ç½®ï¼ˆåŸºäºå®˜æ–¹ AudioCaptureOptionsï¼‰
    const createAudioCaptureOptions = useCallback((settings: AudioProcessingSettings): AudioCaptureOptions => {
        const options: AudioCaptureOptions = {
            // ä½¿ç”¨ LiveKit å®˜æ–¹éŸ³é¢‘å¤„ç†é€‰é¡¹
            autoGainControl: settings.autoGainControl,
            noiseSuppression: settings.noiseSuppression,
            echoCancellation: settings.echoCancellation,
            voiceIsolation: settings.voiceIsolation,
            
            // éŸ³é¢‘è´¨é‡è®¾ç½®
            sampleRate: { ideal: settings.sampleRate },
            channelCount: { ideal: settings.channelCount },
            latency: { ideal: settings.latency },
            
            // è®¾å¤‡é€‰æ‹©ï¼ˆåç»­å¯ä»¥æ‰©å±•ï¼‰
            deviceId: 'default'
        };

        console.log('ğŸ›ï¸ åˆ›å»ºéŸ³é¢‘æ•è·é€‰é¡¹:', options);
        return options;
    }, []);

    // åº”ç”¨éŸ³é¢‘å¤„ç†è®¾ç½®ï¼ˆä½¿ç”¨å®˜æ–¹APIï¼Œä¸é‡å»ºè½¨é“ï¼‰
    const applyAudioProcessing = useCallback(async (newSettings: AudioProcessingSettings) => {
        if (!localParticipant || !room) {
            console.warn('æœ¬åœ°å‚ä¸è€…æˆ–æˆ¿é—´ä¸å­˜åœ¨ï¼Œæ— æ³•åº”ç”¨éŸ³é¢‘è®¾ç½®');
            return false;
        }

        try {
            console.log('ğŸ›ï¸ å¼€å§‹åº”ç”¨éŸ³é¢‘å¤„ç†è®¾ç½®ï¼ˆå®˜æ–¹APIï¼‰:', newSettings);

            // è·å–å½“å‰éŸ³é¢‘å‘å¸ƒçŠ¶æ€
            const audioPublication = localParticipant.getTrackPublication(Track.Source.Microphone);
            const wasEnabled = audioPublication ? !audioPublication.isMuted : false;

            console.log(`ğŸ¤ å½“å‰éº¦å…‹é£çŠ¶æ€: ${wasEnabled ? 'å¯ç”¨' : 'ç¦ç”¨'}`);

            // åˆ›å»ºéŸ³é¢‘æ•è·é€‰é¡¹
            const captureOptions = createAudioCaptureOptions(newSettings);

            // åœæ­¢å½“å‰è½¨é“
            if (audioPublication?.track) {
                console.log('ğŸ›‘ åœæ­¢å½“å‰éŸ³é¢‘è½¨é“');
                audioPublication.track.stop();
                await localParticipant.unpublishTrack(audioPublication.track);
                console.log('ğŸ“¤ å·²å–æ¶ˆå‘å¸ƒå½“å‰éŸ³é¢‘è½¨é“');
                
                // æ¸…ç†å¼•ç”¨
                if (currentTrackRef.current === audioPublication.track) {
                    currentTrackRef.current = null;
                }
            }

            // ç­‰å¾…è½¨é“å®Œå…¨åœæ­¢
            await new Promise(resolve => setTimeout(resolve, 200));

            // ä½¿ç”¨ LiveKit å®˜æ–¹ API åˆ›å»ºæ–°è½¨é“
            console.log('ğŸ”§ ä½¿ç”¨ LiveKit å®˜æ–¹ AudioCaptureOptions åˆ›å»ºéŸ³é¢‘è½¨é“');
            const newAudioTrack = await createLocalAudioTrack(captureOptions);

            console.log('âœ… æ–°éŸ³é¢‘è½¨é“å·²åˆ›å»ºï¼ˆå®˜æ–¹APIï¼‰');

            // ä¿å­˜è½¨é“å¼•ç”¨
            currentTrackRef.current = newAudioTrack;

            // å‘å¸ƒæ–°è½¨é“
            await localParticipant.publishTrack(newAudioTrack, {
                name: 'microphone',
                source: Track.Source.Microphone
            });

            console.log('ğŸ“¤ æ–°éŸ³é¢‘è½¨é“å·²å‘å¸ƒ');

            // æ¢å¤éº¦å…‹é£çŠ¶æ€
            if (wasEnabled) {
                await localParticipant.setMicrophoneEnabled(true);
                console.log('ğŸ¤ éº¦å…‹é£å·²é‡æ–°å¯ç”¨');
            }

            setIsProcessingActive(true);

            // éªŒè¯è®¾ç½®
            setTimeout(() => {
                const finalPublication = localParticipant.getTrackPublication(Track.Source.Microphone);
                if (finalPublication?.track) {
                    const actualSettings = finalPublication.track.mediaStreamTrack.getSettings();
                    console.log('ğŸ” éªŒè¯éŸ³é¢‘è®¾ç½®:', {
                        applied: actualSettings,
                        expected: newSettings,
                        microphoneEnabled: localParticipant.isMicrophoneEnabled
                    });
                }
            }, 1000);

            return true;

        } catch (error) {
            console.error('âŒ åº”ç”¨éŸ³é¢‘å¤„ç†è®¾ç½®å¤±è´¥:', error);
            
            // å°è¯•æ¢å¤åŸºç¡€éŸ³é¢‘åŠŸèƒ½
            try {
                console.log('ğŸ”„ å°è¯•æ¢å¤åŸºç¡€éŸ³é¢‘åŠŸèƒ½...');
                await localParticipant.setMicrophoneEnabled(true);
                console.log('âœ… åŸºç¡€éŸ³é¢‘åŠŸèƒ½å·²æ¢å¤');
            } catch (recoveryError) {
                console.error('âŒ éŸ³é¢‘æ¢å¤å¤±è´¥:', recoveryError);
            }
            
            setIsProcessingActive(false);
            throw error;
        }
    }, [localParticipant, room, createAudioCaptureOptions]);

    // åˆå§‹åŒ–éŸ³é¢‘å¤„ç†ï¼ˆåœ¨æˆ¿é—´è¿æ¥åè‡ªåŠ¨åº”ç”¨ï¼‰
    const initializeAudioProcessing = useCallback(async () => {
        if (initializationRef.current || !localParticipant || !room) {
            return;
        }

        try {
            console.log('ğŸ›ï¸ è‡ªåŠ¨åˆå§‹åŒ–éŸ³é¢‘å¤„ç†...');
            initializationRef.current = true;
            
            // ç­‰å¾…æˆ¿é—´å®Œå…¨è¿æ¥
            await new Promise(resolve => setTimeout(resolve, 1000));
            
            // åº”ç”¨å½“å‰è®¾ç½®
            await applyAudioProcessing(settings);
            
            console.log('âœ… éŸ³é¢‘å¤„ç†è‡ªåŠ¨åˆå§‹åŒ–å®Œæˆ');
        } catch (error) {
            console.error('âŒ éŸ³é¢‘å¤„ç†è‡ªåŠ¨åˆå§‹åŒ–å¤±è´¥:', error);
            initializationRef.current = false;
        }
    }, [localParticipant, room, settings, applyAudioProcessing]);

    // æ›´æ–°å•ä¸ªè®¾ç½®
    const updateSetting = useCallback(async (
        key: keyof AudioProcessingSettings, 
        value: boolean | number
    ) => {
        // é˜²æ­¢é‡å¤åº”ç”¨
        if (applyingSettings.has(key)) {
            console.log(`â³ ${key} è®¾ç½®æ­£åœ¨åº”ç”¨ä¸­ï¼Œè·³è¿‡`);
            return;
        }

        const settingKey = key;
        setApplyingSettings(prev => new Set(prev).add(settingKey));

        try {
            const newSettings = { ...settings, [key]: value };
            
            // ç«‹å³æ›´æ–°æœ¬åœ°çŠ¶æ€
            setSettings(newSettings);
            
            // ä¿å­˜åˆ°æœ¬åœ°å­˜å‚¨
            saveSettings(newSettings);

            // å¯¹äºéŸ³é¢‘å¤„ç†ç›¸å…³è®¾ç½®ï¼Œç«‹å³é‡æ–°åº”ç”¨
            if (key === 'autoGainControl' || 
                key === 'noiseSuppression' || 
                key === 'echoCancellation' || 
                key === 'voiceIsolation' ||
                key === 'sampleRate' ||
                key === 'channelCount' ||
                key === 'latency') {
                console.log(`ğŸ”„ å¼€å§‹åº”ç”¨ ${key} è®¾ç½®: ${value}`);
                await applyAudioProcessing(newSettings);
                console.log(`âœ… ${key} è®¾ç½®å·²åº”ç”¨: ${value}`);
            } else if (key === 'microphoneThreshold') {
                // éº¦å…‹é£é—¨é™è®¾ç½®ä¸éœ€è¦é‡æ–°åˆ›å»ºè½¨é“ï¼Œåªéœ€ä¿å­˜
                console.log(`âœ… éº¦å…‹é£é—¨é™å·²è®¾ç½®ä¸º: ${value}`);
            }

        } catch (error) {
            console.error(`âŒ æ›´æ–° ${key} è®¾ç½®å¤±è´¥:`, error);
            
            // å›æ»šçŠ¶æ€
            setSettings(settings);
            throw error;
        } finally {
            setApplyingSettings(prev => {
                const newSet = new Set(prev);
                newSet.delete(settingKey);
                return newSet;
            });
        }
    }, [settings, applyingSettings, saveSettings, applyAudioProcessing]);

    // æ£€æŸ¥æ˜¯å¦æ­£åœ¨åº”ç”¨è®¾ç½®
    const isApplying = useCallback((key: keyof AudioProcessingSettings) => {
        return applyingSettings.has(key);
    }, [applyingSettings]);

    // é‡ç½®ä¸ºé»˜è®¤è®¾ç½®
    const resetToDefaults = useCallback(async () => {
        console.log('ğŸ”„ é‡ç½®éŸ³é¢‘å¤„ç†è®¾ç½®ä¸ºé»˜è®¤å€¼');
        
        try {
            setSettings(DEFAULT_SETTINGS);
            saveSettings(DEFAULT_SETTINGS);
            await applyAudioProcessing(DEFAULT_SETTINGS);
            console.log('âœ… å·²é‡ç½®ä¸ºé»˜è®¤è®¾ç½®');
        } catch (error) {
            console.error('âŒ é‡ç½®è®¾ç½®å¤±è´¥:', error);
            throw error;
        }
    }, [saveSettings, applyAudioProcessing]);

    // ç›‘å¬æˆ¿é—´è¿æ¥çŠ¶æ€ï¼Œè‡ªåŠ¨åˆå§‹åŒ–
    useEffect(() => {
        if (localParticipant && room?.state === 'connected' && !initializationRef.current) {
            console.log('ğŸ¤ æ£€æµ‹åˆ°æˆ¿é—´å·²è¿æ¥ï¼Œå‡†å¤‡è‡ªåŠ¨åˆå§‹åŒ–éŸ³é¢‘å¤„ç†');
            
            // å»¶è¿Ÿä¸€ç‚¹åˆå§‹åŒ–ï¼Œç¡®ä¿ LiveKit å®Œå…¨å‡†å¤‡å°±ç»ª
            const timer = setTimeout(() => {
                initializeAudioProcessing();
            }, 2000);
            
            return () => clearTimeout(timer);
        }
    }, [localParticipant, room?.state, initializeAudioProcessing]);

    // æ¸…ç†å‡½æ•°
    useEffect(() => {
        return () => {
            console.log('ğŸ§¹ æ¸…ç†éŸ³é¢‘å¤„ç†æ¨¡å—');
            if (currentTrackRef.current) {
                currentTrackRef.current = null;
            }
            initializationRef.current = false;
            setIsProcessingActive(false);
        };
    }, []);

    return {
        settings,
        updateSetting,
        isApplying,
        resetToDefaults,
        isProcessingActive
    };
}