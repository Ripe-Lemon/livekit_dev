// app/hooks/useAudioProcessing.ts
'use client';

import { useState, useEffect, useCallback, useRef } from 'react';
import { useLocalParticipant, useRoomContext } from '@livekit/components-react';
import { Track, LocalAudioTrack } from 'livekit-client';

export interface AudioProcessingSettings {
    autoGainControl: boolean;
    noiseSuppression: boolean;
    echoCancellation: boolean;
    voiceIsolation: boolean;
    microphoneThreshold: number;
    sampleRate: number;
    channelCount: number;
    latency: number;
}

export interface AudioProcessingControls {
    settings: AudioProcessingSettings;
    updateSetting: (key: keyof AudioProcessingSettings, value: boolean | number) => Promise<void>;
    isApplying: (key: keyof AudioProcessingSettings) => boolean;
    resetToDefaults: () => Promise<void>;
    isProcessingActive: boolean;
    isInitialized: boolean;
    audioLevel: number;
}

const DEFAULT_SETTINGS: AudioProcessingSettings = {
    autoGainControl: true,
    noiseSuppression: true,
    echoCancellation: false, // åªèƒ½åœ¨è·å–åŸå§‹æµæ—¶è®¾ç½®
    voiceIsolation: false,
    microphoneThreshold: 0.3,
    sampleRate: 48000,
    channelCount: 1,
    latency: 0.01
};

const STORAGE_KEY = 'livekit_audio_processing_settings';

export function useAudioProcessing(): AudioProcessingControls {
    const { localParticipant } = useLocalParticipant();
    const room = useRoomContext();
    
    // çŠ¶æ€ç®¡ç†
    const [settings, setSettings] = useState<AudioProcessingSettings>(() => {
        if (typeof window === 'undefined') return DEFAULT_SETTINGS;
        try {
            const stored = localStorage.getItem(STORAGE_KEY);
            return stored ? { ...DEFAULT_SETTINGS, ...JSON.parse(stored) } : DEFAULT_SETTINGS;
        } catch {
            return DEFAULT_SETTINGS;
        }
    });
    
    const [applyingSettings, setApplyingSettings] = useState<Set<string>>(new Set());
    const [isProcessingActive, setIsProcessingActive] = useState(false);
    const [isInitialized, setIsInitialized] = useState(false);
    const [audioLevel, setAudioLevel] = useState(0);
    
    // Web Audio API èŠ‚ç‚¹å¼•ç”¨
    const audioContextRef = useRef<AudioContext | null>(null);
    const sourceNodeRef = useRef<MediaStreamAudioSourceNode | null>(null);
    const destinationNodeRef = useRef<MediaStreamAudioDestinationNode | null>(null);
    
    // éŸ³é¢‘å¤„ç†èŠ‚ç‚¹
    const gainNodeRef = useRef<GainNode | null>(null);
    const compressorNodeRef = useRef<DynamicsCompressorNode | null>(null);
    const highpassFilterRef = useRef<BiquadFilterNode | null>(null);
    const lowpassFilterRef = useRef<BiquadFilterNode | null>(null);
    const gateNodeRef = useRef<GainNode | null>(null);
    const analyserNodeRef = useRef<AnalyserNode | null>(null);
    
    // å…¶ä»–å¼•ç”¨
    const originalStreamRef = useRef<MediaStream | null>(null);
    const processedTrackRef = useRef<LocalAudioTrack | null>(null);
    const isInitializingRef = useRef<boolean>(false);
    const animationFrameRef = useRef<number | null>(null);
    const audioDataRef = useRef<Uint8Array | null>(null);

    // ä¿å­˜è®¾ç½®åˆ°æœ¬åœ°å­˜å‚¨
    const saveSettings = useCallback((newSettings: AudioProcessingSettings) => {
        if (typeof window !== 'undefined') {
            try {
                localStorage.setItem(STORAGE_KEY, JSON.stringify(newSettings));
            } catch (error) {
                console.warn('ä¿å­˜éŸ³é¢‘å¤„ç†è®¾ç½®å¤±è´¥:', error);
            }
        }
    }, []);

    // è·å–å½“å‰éŸ³é¢‘è®¾å¤‡ID
    const getCurrentAudioDeviceId = useCallback((): string => {
        if (processedTrackRef.current?.mediaStreamTrack) {
            const trackSettings = processedTrackRef.current.mediaStreamTrack.getSettings();
            if (trackSettings.deviceId) {
                return trackSettings.deviceId;
            }
        }
        
        try {
            const storedDevices = localStorage.getItem('livekit_selected_devices');
            if (storedDevices) {
                const parsed = JSON.parse(storedDevices);
                if (parsed.audioinput) {
                    return parsed.audioinput;
                }
            }
        } catch (error) {
            console.warn('è¯»å–å­˜å‚¨çš„è®¾å¤‡é€‰æ‹©å¤±è´¥:', error);
        }
        
        return 'default';
    }, []);

    // åˆå§‹åŒ– Web Audio API å¤„ç†é“¾
    const initializeAudioProcessingChain = useCallback(async () => {
        console.log('ğŸ›ï¸ åˆå§‹åŒ–éŸ³é¢‘å¤„ç†é“¾...');
        
        try {
            // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
            audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({
                sampleRate: settings.sampleRate,
                latencyHint: 'interactive'
            });

            const audioContext = audioContextRef.current;

            // åˆ›å»ºéŸ³é¢‘å¤„ç†èŠ‚ç‚¹é“¾
            gainNodeRef.current = audioContext.createGain();
            compressorNodeRef.current = audioContext.createDynamicsCompressor();
            highpassFilterRef.current = audioContext.createBiquadFilter();
            lowpassFilterRef.current = audioContext.createBiquadFilter();
            gateNodeRef.current = audioContext.createGain();
            analyserNodeRef.current = audioContext.createAnalyser();
            destinationNodeRef.current = audioContext.createMediaStreamDestination();

            // é…ç½®åˆ†æå™¨
            analyserNodeRef.current.fftSize = 256;
            analyserNodeRef.current.smoothingTimeConstant = 0.8;
            audioDataRef.current = new Uint8Array(analyserNodeRef.current.frequencyBinCount);

            // é…ç½®æ»¤æ³¢å™¨ï¼ˆå™ªå£°æŠ‘åˆ¶ï¼‰
            highpassFilterRef.current.type = 'highpass';
            highpassFilterRef.current.frequency.value = 85; // å»é™¤ä½é¢‘å™ªéŸ³
            highpassFilterRef.current.Q.value = 0.7;

            lowpassFilterRef.current.type = 'lowpass';
            lowpassFilterRef.current.frequency.value = 8000; // å»é™¤é«˜é¢‘å™ªéŸ³
            lowpassFilterRef.current.Q.value = 0.7;

            // é…ç½®å‹ç¼©å™¨ï¼ˆè‡ªåŠ¨å¢ç›Šæ§åˆ¶ï¼‰
            compressorNodeRef.current.threshold.value = -24;
            compressorNodeRef.current.knee.value = 30;
            compressorNodeRef.current.ratio.value = 12;
            compressorNodeRef.current.attack.value = 0.003;
            compressorNodeRef.current.release.value = 0.25;

            // é…ç½®å¢ç›ŠèŠ‚ç‚¹
            gainNodeRef.current.gain.value = 1.0;
            gateNodeRef.current.gain.value = 1.0;

            console.log('âœ… éŸ³é¢‘å¤„ç†é“¾èŠ‚ç‚¹åˆ›å»ºå®Œæˆ');
            return true;
        } catch (error) {
            console.error('âŒ éŸ³é¢‘å¤„ç†é“¾åˆå§‹åŒ–å¤±è´¥:', error);
            throw error;
        }
    }, [settings.sampleRate]);

    // è¿æ¥éŸ³é¢‘å¤„ç†é“¾
    const connectAudioChain = useCallback(() => {
        if (!sourceNodeRef.current || !destinationNodeRef.current) return;

        console.log('ğŸ”— è¿æ¥éŸ³é¢‘å¤„ç†é“¾...');

        // æ„å»ºå¤„ç†é“¾ï¼šè¾“å…¥ â†’ åˆ†æå™¨ â†’ å¢ç›Š â†’ é«˜é€šæ»¤æ³¢ â†’ ä½é€šæ»¤æ³¢ â†’ å‹ç¼©å™¨ â†’ å™ªå£°é—¨ â†’ è¾“å‡º
        sourceNodeRef.current.connect(analyserNodeRef.current!);
        analyserNodeRef.current!.connect(gainNodeRef.current!);
        gainNodeRef.current!.connect(highpassFilterRef.current!);
        highpassFilterRef.current!.connect(lowpassFilterRef.current!);
        lowpassFilterRef.current!.connect(compressorNodeRef.current!);
        compressorNodeRef.current!.connect(gateNodeRef.current!);
        gateNodeRef.current!.connect(destinationNodeRef.current);

        console.log('âœ… éŸ³é¢‘å¤„ç†é“¾å·²è¿æ¥');
    }, []);

    // ğŸ¯ å®æ—¶æ›´æ–°éŸ³é¢‘å¤„ç†å‚æ•°ï¼ˆä¸é‡å»ºè½¨é“ï¼‰
    const updateProcessingChain = useCallback(() => {
        if (!isInitialized) return;

        console.log('âš™ï¸ å®æ—¶æ›´æ–°éŸ³é¢‘å¤„ç†å‚æ•°:', settings);

        // 1. è‡ªåŠ¨å¢ç›Šæ§åˆ¶ï¼ˆé€šè¿‡å‹ç¼©å™¨å®ç°ï¼‰
        if (compressorNodeRef.current) {
            if (settings.autoGainControl) {
                compressorNodeRef.current.threshold.value = -24;
                compressorNodeRef.current.ratio.value = 12;
                compressorNodeRef.current.attack.value = 0.003;
                compressorNodeRef.current.release.value = 0.25;
                gainNodeRef.current!.gain.value = 1.2; // ç¨å¾®æå‡éŸ³é‡
            } else {
                compressorNodeRef.current.threshold.value = -50; // å‡ ä¹ä¸å‹ç¼©
                compressorNodeRef.current.ratio.value = 1;
                gainNodeRef.current!.gain.value = 1.0;
            }
        }

        // 2. å™ªå£°æŠ‘åˆ¶ï¼ˆé€šè¿‡æ»¤æ³¢å™¨å®ç°ï¼‰
        if (highpassFilterRef.current && lowpassFilterRef.current) {
            if (settings.noiseSuppression) {
                highpassFilterRef.current.frequency.value = 85; // è¿‡æ»¤ä½é¢‘å™ªéŸ³
                lowpassFilterRef.current.frequency.value = 8000; // è¿‡æ»¤é«˜é¢‘å™ªéŸ³
                highpassFilterRef.current.Q.value = 0.7;
                lowpassFilterRef.current.Q.value = 0.7;
            } else {
                highpassFilterRef.current.frequency.value = 20; // æœ€å°å€¼
                lowpassFilterRef.current.frequency.value = 20000; // æœ€å¤§å€¼
                highpassFilterRef.current.Q.value = 0.1;
                lowpassFilterRef.current.Q.value = 0.1;
            }
        }

        // 3. è¯­éŸ³éš”ç¦»ï¼ˆå¼ºåŒ–çš„å™ªå£°æŠ‘åˆ¶ï¼‰
        if (settings.voiceIsolation && highpassFilterRef.current && lowpassFilterRef.current && compressorNodeRef.current) {
            highpassFilterRef.current.frequency.value = 120; // æ›´æ¿€è¿›çš„ä½é¢‘è¿‡æ»¤
            lowpassFilterRef.current.frequency.value = 6000; // æ›´æ¿€è¿›çš„é«˜é¢‘è¿‡æ»¤
            compressorNodeRef.current.threshold.value = -18; // æ›´å¼ºçš„å‹ç¼©
            compressorNodeRef.current.ratio.value = 16;
        }

        console.log('âœ… éŸ³é¢‘å¤„ç†å‚æ•°å·²å®æ—¶æ›´æ–°');
    }, [settings, isInitialized]);

    // å®æ—¶éŸ³é¢‘ç›‘æ§å’Œå™ªå£°é—¨æ§
    const startAudioMonitoring = useCallback(() => {
        if (!analyserNodeRef.current || !audioDataRef.current || !gateNodeRef.current || !audioContextRef.current) return;

        const processFrame = () => {
            if (!isProcessingActive || !analyserNodeRef.current || !audioDataRef.current || !gateNodeRef.current || !audioContextRef.current) return;

            // è·å–éŸ³é¢‘æ•°æ®
            analyserNodeRef.current.getByteFrequencyData(audioDataRef.current);
            
            // è®¡ç®—éŸ³é‡çº§åˆ«ï¼ˆRMSï¼‰
            let sum = 0;
            for (let i = 0; i < audioDataRef.current.length; i++) {
                sum += audioDataRef.current[i] * audioDataRef.current[i];
            }
            const rms = Math.sqrt(sum / audioDataRef.current.length);
            const volume = rms / 255; // å½’ä¸€åŒ–åˆ° 0-1

            // æ›´æ–°éŸ³é‡çŠ¶æ€
            setAudioLevel(volume);

            // å®ç°å™ªå£°é—¨æ§ï¼ˆåŸºäºéº¦å…‹é£é—¨é™ï¼‰
            if (volume < settings.microphoneThreshold) {
                // éŸ³é‡ä½äºé—¨é™ï¼Œæ¸è¿›å¼é™ä½éŸ³é‡ï¼ˆé¿å…çªç„¶åˆ‡æ–­ï¼‰
                const targetGain = Math.max(0, volume / settings.microphoneThreshold * 0.1);
                gateNodeRef.current.gain.exponentialRampToValueAtTime(
                    targetGain, 
                    audioContextRef.current.currentTime + 0.1
                );
            } else {
                // éŸ³é‡é«˜äºé—¨é™ï¼Œæ¢å¤æ­£å¸¸éŸ³é‡
                gateNodeRef.current.gain.exponentialRampToValueAtTime(
                    1.0, 
                    audioContextRef.current.currentTime + 0.05
                );
            }

            // ç»§ç»­ä¸‹ä¸€å¸§
            animationFrameRef.current = requestAnimationFrame(processFrame);
        };

        processFrame();
    }, [isProcessingActive, settings.microphoneThreshold]);

    // ğŸ¯ æ ¸å¿ƒï¼šåˆå§‹åŒ–éŸ³é¢‘å¤„ç†ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    const initializeAudioProcessing = useCallback(async () => {
        if (isInitializingRef.current || isInitialized || !localParticipant || !room) {
            return;
        }

        isInitializingRef.current = true;

        try {
            console.log('ğŸ›ï¸ å¼€å§‹åˆå§‹åŒ–éŸ³é¢‘å¤„ç†ç³»ç»Ÿ...');

            // 1. åˆå§‹åŒ– Web Audio API å¤„ç†é“¾
            await initializeAudioProcessingChain();

            // 2. è·å–åŸå§‹éŸ³é¢‘æµï¼ˆåŒ…å«åŸç”Ÿæµè§ˆå™¨å¤„ç†ï¼‰
            const deviceId = getCurrentAudioDeviceId();
            const constraints: MediaStreamConstraints = {
                audio: {
                    deviceId: deviceId === 'default' ? undefined : { exact: deviceId },
                    echoCancellation: false,//settings.echoCancellation, // ğŸ¯ åªåœ¨è·å–æµæ—¶è®¾ç½®
                    sampleRate: { ideal: settings.sampleRate },
                    channelCount: { exact: 1 } // å¼ºåˆ¶å•å£°é“
                }
            };

            console.log('ğŸ¤ è·å–åŸå§‹éŸ³é¢‘æµï¼Œçº¦æŸ:', constraints);
            const originalStream = await navigator.mediaDevices.getUserMedia(constraints);
            originalStreamRef.current = originalStream;

            // 3. è¿æ¥åˆ° Web Audio API å¤„ç†é“¾
            sourceNodeRef.current = audioContextRef.current!.createMediaStreamSource(originalStream);
            connectAudioChain();

            // 4. è·å–å¤„ç†åçš„éŸ³é¢‘æµ
            const processedStream = destinationNodeRef.current!.stream;

            // 5. ğŸ¯ å…³é”®ï¼šä»å¤„ç†åçš„æµåˆ›å»ºéŸ³é¢‘è½¨é“ï¼ˆåªåˆ›å»ºä¸€æ¬¡ï¼‰
            const processedAudioTrack = processedStream.getAudioTracks()[0];
            if (!processedAudioTrack) {
                throw new Error('æ— æ³•è·å–å¤„ç†åçš„éŸ³é¢‘è½¨é“');
            }

            // åŒ…è£…ä¸º LocalAudioTrack
            processedTrackRef.current = new LocalAudioTrack(
                processedAudioTrack,
                undefined,
                false // ä¸æ˜¯æ¥è‡ªå±å¹•å…±äº«
            );

            // 6. ğŸ¯ ä¸€æ¬¡æ€§å‘å¸ƒåˆ° LiveKitï¼ˆåç»­ä¸å†é‡å»ºï¼‰
            await localParticipant.publishTrack(processedTrackRef.current, {
                name: 'microphone',
                source: Track.Source.Microphone,
                stopMicTrackOnMute: true
            });

            // 7. åº”ç”¨åˆå§‹å¤„ç†è®¾ç½®
            updateProcessingChain();

            // 8. å¼€å§‹å®æ—¶ç›‘æ§
            startAudioMonitoring();

            setIsProcessingActive(true);
            setIsInitialized(true);

            console.log('âœ… éŸ³é¢‘å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ - è½¨é“å·²å‘å¸ƒï¼Œåç»­åªæ›´æ–°å¤„ç†å‚æ•°');

        } catch (error) {
            console.error('âŒ éŸ³é¢‘å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥:', error);
            setIsProcessingActive(false);
            throw error;
        } finally {
            isInitializingRef.current = false;
        }
    }, [
        localParticipant, 
        room, 
        isInitialized, 
        settings.echoCancellation, 
        settings.sampleRate, 
        settings.latency,
        initializeAudioProcessingChain,
        getCurrentAudioDeviceId,
        connectAudioChain,
        updateProcessingChain,
        startAudioMonitoring
    ]);

    // ğŸ¯ æ›´æ–°å•ä¸ªè®¾ç½®ï¼ˆåªæ›´æ–°å¤„ç†å‚æ•°ï¼Œä¸é‡å»ºè½¨é“ï¼‰
    const updateSetting = useCallback(async (
        key: keyof AudioProcessingSettings, 
        value: boolean | number
    ) => {
        if (applyingSettings.has(key)) {
            console.log(`â³ ${key} è®¾ç½®æ­£åœ¨åº”ç”¨ä¸­ï¼Œè·³è¿‡`);
            return;
        }

        const settingKey = key;
        setApplyingSettings(prev => new Set(prev).add(settingKey));

        try {
            const newSettings = { ...settings, [key]: value };
            
            // ç«‹å³æ›´æ–°æœ¬åœ°çŠ¶æ€å’Œå­˜å‚¨
            setSettings(newSettings);
            saveSettings(newSettings);

            // ğŸ¯ å…³é”®ï¼šåªæ›´æ–°å¤„ç†å‚æ•°ï¼Œä¸é‡å»ºè½¨é“
            if (isInitialized) {
                console.log(`ğŸ”„ å®æ—¶æ›´æ–° ${key} è®¾ç½®: ${value}`);
                
                // ç›´æ¥è°ƒç”¨å¤„ç†é“¾æ›´æ–°ï¼Œä½¿ç”¨æ–°è®¾ç½®
                if (key === 'autoGainControl' && compressorNodeRef.current && gainNodeRef.current) {
                    if (value) {
                        compressorNodeRef.current.threshold.value = -24;
                        compressorNodeRef.current.ratio.value = 12;
                        gainNodeRef.current.gain.value = 1.2;
                    } else {
                        compressorNodeRef.current.threshold.value = -50;
                        compressorNodeRef.current.ratio.value = 1;
                        gainNodeRef.current.gain.value = 1.0;
                    }
                } else if (key === 'noiseSuppression' && highpassFilterRef.current && lowpassFilterRef.current) {
                    if (value) {
                        highpassFilterRef.current.frequency.value = 85;
                        lowpassFilterRef.current.frequency.value = 8000;
                    } else {
                        highpassFilterRef.current.frequency.value = 20;
                        lowpassFilterRef.current.frequency.value = 20000;
                    }
                } else if (key === 'voiceIsolation') {
                    // è¯­éŸ³éš”ç¦»éœ€è¦å®Œæ•´æ›´æ–°å¤„ç†é“¾
                    updateProcessingChain();
                }
                // microphoneThreshold ä¼šåœ¨å®æ—¶ç›‘æ§ä¸­è‡ªåŠ¨ä½¿ç”¨æ–°å€¼
                
                console.log(`âœ… ${key} è®¾ç½®å·²å®æ—¶ç”Ÿæ•ˆ: ${value}`);
            } else {
                console.log(`ğŸ’¾ ${key} è®¾ç½®å·²ä¿å­˜ï¼Œå°†åœ¨åˆå§‹åŒ–æ—¶ç”Ÿæ•ˆ: ${value}`);
            }

        } catch (error) {
            console.error(`âŒ æ›´æ–° ${key} è®¾ç½®å¤±è´¥:`, error);
            setSettings(settings); // å›æ»š
            throw error;
        } finally {
            setApplyingSettings(prev => {
                const newSet = new Set(prev);
                newSet.delete(settingKey);
                return newSet;
            });
        }
    }, [settings, applyingSettings, saveSettings, isInitialized, updateProcessingChain]);

    // æ£€æŸ¥æ˜¯å¦æ­£åœ¨åº”ç”¨è®¾ç½®
    const isApplying = useCallback((key: keyof AudioProcessingSettings) => {
        return applyingSettings.has(key);
    }, [applyingSettings]);

    // é‡ç½®ä¸ºé»˜è®¤è®¾ç½®
    const resetToDefaults = useCallback(async () => {
        console.log('ğŸ”„ é‡ç½®éŸ³é¢‘å¤„ç†è®¾ç½®ä¸ºé»˜è®¤å€¼');
        
        try {
            setSettings(DEFAULT_SETTINGS);
            saveSettings(DEFAULT_SETTINGS);
            
            if (isInitialized) {
                // ğŸ¯ åªæ›´æ–°å¤„ç†å‚æ•°ï¼Œä¸é‡å»ºè½¨é“
                updateProcessingChain();
            }
            
            console.log('âœ… å·²é‡ç½®ä¸ºé»˜è®¤è®¾ç½®');
        } catch (error) {
            console.error('âŒ é‡ç½®è®¾ç½®å¤±è´¥:', error);
            throw error;
        }
    }, [saveSettings, isInitialized, updateProcessingChain]);

    // ç›‘å¬æˆ¿é—´è¿æ¥çŠ¶æ€ï¼Œè‡ªåŠ¨åˆå§‹åŒ–
    useEffect(() => {
        if (!localParticipant || !room || room.state !== 'connected') {
            return;
        }

        if (isInitialized || isInitializingRef.current) {
            return;
        }

        console.log('ğŸ›ï¸ æˆ¿é—´å·²è¿æ¥ï¼Œå‡†å¤‡åˆå§‹åŒ–éŸ³é¢‘å¤„ç†');

        const timer = setTimeout(() => {
            initializeAudioProcessing().catch(error => {
                console.error('âŒ éŸ³é¢‘å¤„ç†è‡ªåŠ¨åˆå§‹åŒ–å¤±è´¥:', error);
            });
        }, 1500);

        return () => clearTimeout(timer);
    }, [localParticipant, room, room?.state, isInitialized, initializeAudioProcessing]);

    // æ¸…ç†å‡½æ•°
    useEffect(() => {
        return () => {
            console.log('ğŸ§¹ æ¸…ç†éŸ³é¢‘å¤„ç†æ¨¡å—');
            
            // åœæ­¢å®æ—¶ç›‘æ§
            if (animationFrameRef.current) {
                cancelAnimationFrame(animationFrameRef.current);
                animationFrameRef.current = null;
            }
            
            // æ¸…ç† Web Audio API èŠ‚ç‚¹
            if (sourceNodeRef.current) {
                sourceNodeRef.current.disconnect();
                sourceNodeRef.current = null;
            }
            
            // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
            if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
                audioContextRef.current.close();
                audioContextRef.current = null;
            }
            
            // åœæ­¢åŸå§‹æµ
            if (originalStreamRef.current) {
                originalStreamRef.current.getTracks().forEach(track => track.stop());
                originalStreamRef.current = null;
            }
            
            // æ¸…ç†è½¨é“å¼•ç”¨
            if (processedTrackRef.current) {
                processedTrackRef.current = null;
            }
            
            // é‡ç½®æ‰€æœ‰èŠ‚ç‚¹å¼•ç”¨
            gainNodeRef.current = null;
            compressorNodeRef.current = null;
            highpassFilterRef.current = null;
            lowpassFilterRef.current = null;
            gateNodeRef.current = null;
            analyserNodeRef.current = null;
            destinationNodeRef.current = null;
            audioDataRef.current = null;
            
            setIsProcessingActive(false);
            setIsInitialized(false);
            isInitializingRef.current = false;
        };
    }, []);

    return {
        settings,
        updateSetting,
        isApplying,
        resetToDefaults,
        isProcessingActive,
        isInitialized,
        audioLevel
    };
}