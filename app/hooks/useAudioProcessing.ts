// app/hooks/useAudioProcessing.ts
'use client';

import { useState, useEffect, useCallback, useRef } from 'react';
import { useLocalParticipant, useRoomContext } from '@livekit/components-react';
import { Track, LocalAudioTrack } from 'livekit-client';

export interface AudioProcessingSettings {
    preamp: number; // å‰ç½®å¢ç›Š
    postamp: number;
    autoGainControl: boolean;
    noiseSuppression: boolean;
    echoCancellation: boolean;
    vadEnabled: boolean;
    vadAttackTime: number;      // è¯­éŸ³æŒç»­è¶…è¿‡æ­¤æ—¶é•¿æ‰å¼€é—¨ (ms)
    vadReleaseTime: number;     // è¯­éŸ³ä½äºé˜ˆå€¼è¶…è¿‡æ­¤æ—¶é•¿æ‰å…³é—¨ (ms)
    vadActivationThreshold: number;   // æ¿€æ´»VADçš„éŸ³é‡é˜ˆå€¼ (ä¸Šé—¨é™)
    vadDeactivationThreshold: number; // åœæ­¢VADçš„éŸ³é‡é˜ˆå€¼ (ä¸‹é—¨é™)
    sampleRate: number;
    channels: number;
}

export interface AudioProcessingControls {
    settings: AudioProcessingSettings;
    updateSetting: (key: keyof AudioProcessingSettings, value: boolean | number) => void;
    isApplying: (key: keyof AudioProcessingSettings) => boolean;
    isProcessingActive: boolean;
    isInitialized: boolean;
    audioLevel: number;
    isVADActive: boolean;
}

const DEFAULT_SETTINGS: Omit<AudioProcessingSettings, 'echoCancellation'> = {
    preamp: 1.0,
    postamp: 3.5,
    autoGainControl: false,
    noiseSuppression: false,
    vadEnabled: true,
    vadAttackTime: 40, // é»˜è®¤40ms
    vadReleaseTime: 300, // é»˜è®¤300ms
    vadActivationThreshold: 0.35,
    vadDeactivationThreshold: 0.25,
    sampleRate: 48000,
    channels: 1,
};

const STORAGE_KEY = 'livekit_audio_processing_settings_custom_vad_V3';
type StoredSettings = Partial<AudioProcessingSettings & { microphoneThreshold?: number }>;

export function useAudioProcessing(): AudioProcessingControls {
    const { localParticipant } = useLocalParticipant();
    const room = useRoomContext();
    
    // çŠ¶æ€ç®¡ç†
    const [settings, setSettings] = useState<AudioProcessingSettings>(() => {
        if (typeof window === 'undefined') return { ...DEFAULT_SETTINGS, echoCancellation: false };
        try {
            const stored = localStorage.getItem(STORAGE_KEY);
            if (stored) {
                const parsed: StoredSettings = JSON.parse(stored);
                // ç§»é™¤å·²åºŸå¼ƒçš„ microphoneThreshold å±æ€§
                delete parsed.microphoneThreshold;
                return { ...DEFAULT_SETTINGS, echoCancellation: false, ...parsed };
            }
            return { ...DEFAULT_SETTINGS, echoCancellation: false };
        } catch {
            return { ...DEFAULT_SETTINGS, echoCancellation: false };
        }
    });
    
    const [applyingSettings, setApplyingSettings] = useState<Set<string>>(new Set());
    const [isProcessingActive, setIsProcessingActive] = useState(false);
    const [isInitialized, setIsInitialized] = useState(false);
    const [audioLevel, setAudioLevel] = useState(0);
    
    const [isVADActive, setIsVADActive] = useState(false);

    const settingsRef = useRef(settings);

    // Web Audio API èŠ‚ç‚¹å¼•ç”¨
    const audioContextRef = useRef<AudioContext | null>(null);
    const sourceNodeRef = useRef<MediaStreamAudioSourceNode | null>(null);
    const destinationNodeRef = useRef<MediaStreamAudioDestinationNode | null>(null);
    
    // éŸ³é¢‘å¤„ç†èŠ‚ç‚¹
    const gateNodeRef = useRef<GainNode | null>(null);
    const delayNodeRef = useRef<DelayNode | null>(null);
    const vadStateRef = useRef({
        isSpeaking: false,
        attackTimeout: null as NodeJS.Timeout | null,
        releaseTimeout: null as NodeJS.Timeout | null,
    });
    const analyserNodeRef = useRef<AnalyserNode | null>(null);
    const preampNodeRef = useRef<GainNode | null>(null);
    const postampNodeRef = useRef<GainNode | null>(null);
    const vadAudioContextRef = useRef<AudioContext | null>(null);

    // å…¶ä»–å¼•ç”¨
    const originalStreamRef = useRef<MediaStream | null>(null);
    const processedTrackRef = useRef<LocalAudioTrack | null>(null);
    const isInitializingRef = useRef<boolean>(false);
    const animationFrameRef = useRef<number | null>(null);
    const audioDataRef = useRef<Uint8Array | null>(null);
    const currentDeviceIdRef = useRef<string | null>(null);

    // ä¿å­˜è®¾ç½®åˆ°æœ¬åœ°å­˜å‚¨
    const saveSettings = useCallback((newSettings: AudioProcessingSettings) => {
        if (typeof window !== 'undefined') {
            try {
                localStorage.setItem(STORAGE_KEY, JSON.stringify(newSettings));
            } catch (error) {
                console.warn('ä¿å­˜éŸ³é¢‘å¤„ç†è®¾ç½®å¤±è´¥:', error);
            }
        }
    }, []);

    // è·å–å½“å‰éŸ³é¢‘è®¾å¤‡ID
    const getCurrentAudioDeviceId = useCallback((): string => {
        // 1. ä¼˜å…ˆæ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ‰‹åŠ¨é€‰æ‹©äº†è®¾å¤‡
        try {
            const storedDevices = localStorage.getItem('livekit_selected_devices');
            if (storedDevices) {
                const parsed = JSON.parse(storedDevices);
                if (parsed.audioinput && parsed.audioinput !== 'default') {
                    console.log('ğŸ¤ ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„è®¾å¤‡:', parsed.audioinput);
                    return parsed.audioinput;
                }
            }
        } catch (error) {
            console.warn('è¯»å–å­˜å‚¨çš„è®¾å¤‡é€‰æ‹©å¤±è´¥:', error);
        }
        
        // 2. æ£€æŸ¥å½“å‰è½¨é“è®¾å¤‡
        if (processedTrackRef.current?.mediaStreamTrack) {
            const trackSettings = processedTrackRef.current.mediaStreamTrack.getSettings();
            if (trackSettings.deviceId) {
                console.log('ğŸ¤ ä½¿ç”¨å½“å‰è½¨é“è®¾å¤‡:', trackSettings.deviceId);
                return trackSettings.deviceId;
            }
        }
        
        // 3. ğŸ¯ é»˜è®¤è¿”å› 'default' è®¾å¤‡ï¼ˆç³»ç»Ÿé»˜è®¤éº¦å…‹é£ï¼‰
        console.log('ğŸ¤ ä½¿ç”¨ç³»ç»Ÿé»˜è®¤éº¦å…‹é£è®¾å¤‡');
        return 'default';
    }, []);

    const controlGate = useCallback((action: 'open' | 'close') => {
        if (gateNodeRef.current && audioContextRef.current?.state === 'running') {
            const gateNode = gateNodeRef.current;
            const audioContext = audioContextRef.current;
            const now = audioContext.currentTime;
            gateNode.gain.cancelScheduledValues(now);
            gateNode.gain.setValueAtTime(gateNode.gain.value, now);
            if (action === 'open') {
                gateNode.gain.exponentialRampToValueAtTime(1.0, now + 0.01);
            } else {
                gateNode.gain.exponentialRampToValueAtTime(0.0001, now + 0.2); // å…³é—¨é€Ÿåº¦å¯ä»¥æ…¢ä¸€äº›
            }
        }
    }, []);

    // åˆå§‹åŒ– Web Audio API å¤„ç†é“¾
    const initializeAudioProcessingChain = useCallback(async () => {
        console.log('ğŸ›ï¸ åˆå§‹åŒ–éŸ³é¢‘å¤„ç†é“¾...');
        
        try {
            // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
            audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({
                sampleRate: settings.sampleRate,
                latencyHint: 'interactive'
            });

            const audioContext = audioContextRef.current;

            // åˆ›å»ºéŸ³é¢‘å¤„ç†èŠ‚ç‚¹é“¾
            gateNodeRef.current = audioContext.createGain();
            analyserNodeRef.current = audioContext.createAnalyser();
            destinationNodeRef.current = audioContext.createMediaStreamDestination();
            postampNodeRef.current = audioContext.createGain();
            postampNodeRef.current.gain.value = settings.postamp || 1.0;
            delayNodeRef.current = audioContext.createDelay(0.5); // æœ€å¤§å»¶è¿Ÿ0.5ç§’
            delayNodeRef.current.delayTime.value = 0.1; // é»˜è®¤å»¶è¿Ÿ100ms

            gateNodeRef.current.gain.value = 0.0;

            // é…ç½®åˆ†æå™¨
            analyserNodeRef.current.fftSize = 256;
            analyserNodeRef.current.smoothingTimeConstant = 0.8;
            audioDataRef.current = new Uint8Array(analyserNodeRef.current.frequencyBinCount);
            
            console.log('âœ… éŸ³é¢‘å¤„ç†é“¾èŠ‚ç‚¹åˆ›å»ºå®Œæˆ');
            return true;
        } catch (error) {
            console.error('âŒ éŸ³é¢‘å¤„ç†é“¾åˆå§‹åŒ–å¤±è´¥:', error);
            throw error;
        }
    }, [settings.sampleRate, settings.postamp]);

    // è¿æ¥éŸ³é¢‘å¤„ç†é“¾
    const connectAudioChain = useCallback(() => {
        if (!sourceNodeRef.current || !destinationNodeRef.current) return;

        console.log('ğŸ”— è¿æ¥éŸ³é¢‘å¤„ç†é“¾...');

        // è¾“å…¥ -> å‰ç½®æ”¾å¤§/å•å£°é“ -> VADé—¨ -> [åˆ†æå™¨ -> å¢ç›Š -> æ»¤æ³¢ -> å‹ç¼©] -> è¾“å‡º
        // è¿™æ ·å¯ä»¥ç¡®ä¿åªå¯¹é€šè¿‡VADçš„çº¯è¯­éŸ³è¿›è¡Œå¤„ç†ï¼Œæ›´é«˜æ•ˆã€æ•ˆæœæ›´å¥½ã€‚

        const source = sourceNodeRef.current; // è¿™æ˜¯ç»è¿‡å‰ç½®å¤„ç†çš„æº
        const analyser = analyserNodeRef.current!; // VADæ£€æµ‹å™¨
        const gate = gateNodeRef.current!;
        const delay = delayNodeRef.current!;
        const postamp = postampNodeRef.current!;
        const destination = destinationNodeRef.current!;

        // æ–­å¼€æ‰€æœ‰æ—§è¿æ¥ï¼Œä»¥é˜²ä¸‡ä¸€
        source.disconnect();
        gate.disconnect();
        postamp.disconnect();
        analyser.disconnect();

        // æ–°çš„è¿æ¥é¡ºåº
        source.connect(postamp);
        postamp.connect(analyser);
        analyser.connect(delay); 
        delay.connect(gate);
        gate.connect(destination);

        console.log('âœ… è‡ªå®šä¹‰VADå¤„ç†é“¾å·²è¿æ¥');
    }, []);

    // å®æ—¶éŸ³é¢‘ç›‘æ§å’Œå™ªå£°é—¨æ§
    const startAudioMonitoring = useCallback(() => {
        if (!analyserNodeRef.current || !audioDataRef.current) return;
        const analyser = analyserNodeRef.current;
        const audioData = new Uint8Array(analyser.fftSize);
        
        const processFrame = () => {
            if (animationFrameRef.current === null) return; // æ£€æŸ¥æ˜¯å¦å·²è¢«æ¸…ç†

            if (analyser) {
                // ğŸ¯ 2. æ”¹ç”¨ getByteTimeDomainData è·å–åŸå§‹æ³¢å½¢æ•°æ®
                analyser.getByteTimeDomainData(audioData);

                // ğŸ¯ 3. å¯»æ‰¾æ³¢å½¢ä¸­çš„å³°å€¼ä½œä¸ºå½“å‰éŸ³é‡
                let peak = 0;
                // audioData ä¸­çš„å€¼èŒƒå›´æ˜¯ 0-255ï¼Œé™éŸ³æ—¶æ˜¯ 128
                for (let i = 0; i < audioData.length; i++) {
                    const value = Math.abs(audioData[i] - 128); // è®¡ç®—åç¦»ä¸­å¿ƒçš„æŒ¯å¹…
                    if (value > peak) {
                        peak = value;
                    }
                }
                
                // å°†å³°å€¼ (0-128) å½’ä¸€åŒ–åˆ° 0-1 çš„èŒƒå›´
                const volume = peak / 128.0;
                setAudioLevel(volume);

                // --- å¸¦æœ‰è¿Ÿæ»åŠŸèƒ½çš„è‡ªå®šä¹‰VADçŠ¶æ€æœº ---
                const currentSettings = settingsRef.current; 
                //if (currentSettings.vadEnabled) {
                    if (volume > currentSettings.vadActivationThreshold) {
                        // --- éŸ³é‡é«˜äºâ€œä¸Šé—¨é™â€ (æ¿€æ´») ---
                        // å¦‚æœæœ‰å…³é—­é—¨çš„è®¡æ—¶å™¨ï¼Œç«‹å³å–æ¶ˆå®ƒï¼Œå› ä¸ºæˆ‘ä»¬è¿˜åœ¨è¯´è¯
                        if (vadStateRef.current.releaseTimeout) {
                            clearTimeout(vadStateRef.current.releaseTimeout);
                            vadStateRef.current.releaseTimeout = null;
                        }
                        // å¦‚æœå½“å‰æ˜¯â€œä¸è¯´è¯â€çŠ¶æ€ï¼Œå¹¶ä¸”æ²¡æœ‰æ­£åœ¨è®¡æ—¶çš„â€œå¼€é—¨â€ä»»åŠ¡
                        if (!vadStateRef.current.isSpeaking && !vadStateRef.current.attackTimeout) {
                            // å¼€å§‹ä¸€ä¸ªâ€œå¼€é—¨â€è®¡æ—¶
                            vadStateRef.current.attackTimeout = setTimeout(() => {
                                controlGate('open');
                                setIsVADActive(true);
                                vadStateRef.current.isSpeaking = true;
                                vadStateRef.current.attackTimeout = null;
                            }, currentSettings.vadAttackTime);
                        }
                    } else if (volume < currentSettings.vadDeactivationThreshold) {
                        // --- éŸ³é‡ä½äºâ€œä¸‹é—¨é™â€ (å‡†å¤‡å…³é—­) ---
                        // å¦‚æœæœ‰â€œå¼€é—¨â€çš„è®¡æ—¶å™¨ï¼Œç«‹å³å–æ¶ˆå®ƒï¼Œå› ä¸ºå£°éŸ³å·²ç»å˜å°äº†
                        if (vadStateRef.current.attackTimeout) {
                            clearTimeout(vadStateRef.current.attackTimeout);
                            vadStateRef.current.attackTimeout = null;
                        }
                        // å¦‚æœå½“å‰æ˜¯â€œè¯´è¯â€çŠ¶æ€ï¼Œå¹¶ä¸”æ²¡æœ‰æ­£åœ¨è®¡æ—¶çš„â€œå…³é—¨â€ä»»åŠ¡
                        if (vadStateRef.current.isSpeaking && !vadStateRef.current.releaseTimeout) {
                            // å¼€å§‹ä¸€ä¸ªâ€œå…³é—¨â€è®¡æ—¶
                            vadStateRef.current.releaseTimeout = setTimeout(() => {
                                controlGate('close');
                                setIsVADActive(false);
                                vadStateRef.current.isSpeaking = false;
                                vadStateRef.current.releaseTimeout = null;
                            }, currentSettings.vadReleaseTime);
                        }
                    }
                    // å¦‚æœéŸ³é‡åœ¨ä¸Šé—¨é™å’Œä¸‹é—¨é™ä¹‹é—´ï¼Œåˆ™â€œç»´æŒç°çŠ¶â€ï¼Œä¸åšä»»ä½•æ“ä½œ
                //}
            }
            animationFrameRef.current = requestAnimationFrame(processFrame);
        };
        processFrame();

        // è¿”å›æ¸…ç†å‡½æ•°
        return () => {
            if (animationFrameRef.current) {
                cancelAnimationFrame(animationFrameRef.current);
                animationFrameRef.current = null;
            }
            if (vadStateRef.current.attackTimeout) clearTimeout(vadStateRef.current.attackTimeout);
            if (vadStateRef.current.releaseTimeout) clearTimeout(vadStateRef.current.releaseTimeout);
        };
    // ç§»é™¤äº†å¯¹ settings çš„ä¾èµ–ï¼Œä½¿æ­¤å‡½æ•°æ›´ç¨³å®š
    }, [controlGate]);

    // ğŸ¯ æ–°å¢ï¼šç¡®ä¿ AudioContext å¤„äºè¿è¡ŒçŠ¶æ€çš„å‡½æ•°
    const ensureAudioContextRunning = useCallback(async () => {
        if (audioContextRef.current && audioContextRef.current.state === 'suspended') {
            console.log('AudioContext is suspended, attempting to resume...');
            try {
                await audioContextRef.current.resume();
                console.log('âœ… AudioContext resumed successfully.');
            } catch (e) {
                console.error('âŒ Failed to resume AudioContext:', e);
            }
        }
    }, []);

    // ğŸ¯ æ ¸å¿ƒï¼šåˆå§‹åŒ–éŸ³é¢‘å¤„ç†ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    const initializeAudioProcessing = useCallback(async (deviceId: string) => {
        if (isInitializingRef.current || isInitialized || !localParticipant || !room) return;
        isInitializingRef.current = true;
        
        try {
            console.log('ğŸ›ï¸ å¼€å§‹åˆå§‹åŒ–è‡ªå®šä¹‰éŸ³é¢‘å¤„ç†ç³»ç»Ÿ...');
            // æ¸…ç†ç°æœ‰è½¨é“...
            const existingAudioPublications = Array.from(localParticipant.audioTrackPublications.values());
            for (const pub of existingAudioPublications) {
                if (pub.track) {
                    await localParticipant.unpublishTrack(pub.track);
                }
            }
            await new Promise(resolve => setTimeout(resolve, 200));

            // æ­¥éª¤2ï¼šåˆå§‹åŒ– Web Audio API å¤„ç†é“¾
            await initializeAudioProcessingChain();

            // æ­¥éª¤3ï¼šè·å–åŸå§‹éŸ³é¢‘æµï¼ˆä½¿ç”¨é»˜è®¤è®¾å¤‡ï¼Œç¦ç”¨æµè§ˆå™¨å¤„ç†ï¼‰
            const deviceId = getCurrentAudioDeviceId();
            const constraints: MediaStreamConstraints = {
                audio: {
                    deviceId: deviceId === 'default' ? undefined : { exact: deviceId },
                    echoCancellation: settings.echoCancellation,
                    noiseSuppression: settings.noiseSuppression,
                    autoGainControl: false,
                    sampleRate: { ideal: settings.sampleRate },
                    channelCount: { exact: 1 }
                }
            };

            console.log('ğŸ¤ è·å–å¸¦æœ‰æµè§ˆå™¨åŸç”Ÿå¤„ç†çš„éŸ³é¢‘æµ:', constraints.audio);
            const originalStream = await navigator.mediaDevices.getUserMedia(constraints);
            originalStreamRef.current = originalStream;
            
            // ğŸ¯ æ–°å¢æ­¥éª¤ 3.5ï¼šåˆ›å»ºå‰ç½®å¤„ç†é˜¶æ®µ (å¢ç›Š + å•å£°é“)
            console.log('ğŸ”Š åº”ç”¨å‰ç½®å¢ç›Šå’Œå•å£°é“è½¬æ¢...');
            const sourceForPreamp = audioContextRef.current!.createMediaStreamSource(originalStream);
            
            const preampNode = audioContextRef.current!.createGain();
            // ä½¿ç”¨settingsä¸­çš„å¢ç›Šå€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é»˜è®¤ä¸º1.0
            preampNodeRef.current = preampNode;
            preampNode.gain.value = settings.preamp || 1.0; 

            // å¼ºåˆ¶å°†éŸ³é¢‘æ··åˆä¸ºå•å£°é“ï¼Œè§£å†³åªæœ‰å·¦å£°é“çš„é—®é¢˜
            preampNode.channelCount = 1;
            preampNode.channelCountMode = 'explicit';
            preampNode.channelInterpretation = 'speakers';

            // åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ç›®æ ‡èŠ‚ç‚¹ï¼Œä»¥ç”ŸæˆåŒ…å«å‰ç½®å¤„ç†æ•ˆæœçš„æ–°éŸ³é¢‘æµ
            const preampDestinationNode = audioContextRef.current!.createMediaStreamDestination();
            sourceForPreamp.connect(preampNode);
            preampNode.connect(preampDestinationNode);

            // è¿™å°±æ˜¯ç»è¿‡å‰ç½®å¤„ç†ï¼ˆå¢ç›Šæ”¾å¤§+è½¬ä¸ºå•å£°é“ï¼‰çš„æ–°éŸ³é¢‘æµ
            const boostedAndMonoStream = preampDestinationNode.stream;
            
            await ensureAudioContextRunning();

            // ğŸ¯ ä¿®æ”¹æ­¥éª¤ 4ï¼šä½¿ç”¨ç»è¿‡å‰ç½®å¤„ç†çš„æµæ¥è¿æ¥ä¸»å¤„ç†é“¾
            sourceNodeRef.current = audioContextRef.current!.createMediaStreamSource(boostedAndMonoStream);
            connectAudioChain();

            if (!settings.vadEnabled) {
                controlGate('open');
            }
            
            // å‘å¸ƒè½¨é“
            const processedStream = destinationNodeRef.current!.stream;
            const processedAudioTrack = processedStream.getAudioTracks()[0];
            processedTrackRef.current = new LocalAudioTrack(processedAudioTrack, undefined, false);
            await localParticipant.publishTrack(processedTrackRef.current, { name: 'custom-microphone-vad', source: Track.Source.Microphone, stopMicTrackOnMute: false });

            // æ›´æ–°å’Œå¯åŠ¨ç›‘æ§
            startAudioMonitoring();

            setIsProcessingActive(true);
            setIsInitialized(true);
            console.log('âœ… è‡ªå®šä¹‰éŸ³é¢‘å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ');
        } catch (error) {
            console.error('âŒ è‡ªå®šä¹‰éŸ³é¢‘å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥:', error);
            setIsProcessingActive(false);
            throw error;
        } finally {
            isInitializingRef.current = false;
        }
    }, [
        // ä¾èµ–é¡¹ä¿æŒä¸å˜
        localParticipant, 
        room, 
        isInitialized, 
        settings,
        controlGate,
        initializeAudioProcessingChain,
        getCurrentAudioDeviceId,
        connectAudioChain,
        startAudioMonitoring,
        ensureAudioContextRunning
    ]);

    // ğŸ¯ æ›´æ–°å•ä¸ªè®¾ç½®ï¼ˆåªæ›´æ–°å¤„ç†å‚æ•°ï¼Œä¸é‡å»ºè½¨é“ï¼‰
    const updateSetting = useCallback((key: keyof AudioProcessingSettings, value: boolean | number) => {
        setSettings(prevSettings => {
            const newSettings = { ...prevSettings, [key]: value };
            saveSettings(newSettings);
            return newSettings;
        });
    }, [saveSettings]);

    // ğŸ¯ 2. åˆ›å»ºä¸€ä¸ªuseEffectï¼Œåœ¨æ¯æ¬¡settingså˜åŒ–æ—¶ï¼Œéƒ½å»æ›´æ–°refçš„å€¼
    // è¿™ç¡®ä¿äº†settingsRef.currentæ°¸è¿œæ˜¯æœ€æ–°çš„
    useEffect(() => {
        settingsRef.current = settings;
    }, [settings]);

    // æ–°å¢ä¸€ä¸ªuseEffectæ¥å¤„ç†éœ€è¦é‡å»ºç®¡çº¿çš„è®¾ç½®
    useEffect(() => {
        // è¿™ä¸ªeffectåªåœ¨isInitializedä¹‹åï¼Œå½“echoCancellationå˜åŒ–æ—¶è§¦å‘
        // å®ƒä¼šè§¦å‘å®Œæ•´çš„ç®¡çº¿é‡å»ºæµç¨‹
        if (isInitialized && room && localParticipant) {
            // é‡æ–°åˆå§‹åŒ–ä¼šè¯»å–æœ€æ–°çš„ settings.echoCancellation å€¼
            initializeAudioProcessing(currentDeviceIdRef.current || 'default');
        }
    }, [settings.echoCancellation]); // åªä¾èµ–è¿™ä¸€ä¸ªä¼šå¼ºåˆ¶é‡å»ºçš„å‚æ•°

    // æ£€æŸ¥æ˜¯å¦æ­£åœ¨åº”ç”¨è®¾ç½®
    const isApplying = useCallback((key: keyof AudioProcessingSettings) => {
        return applyingSettings.has(key);
    }, [applyingSettings, settings.autoGainControl, settings.noiseSuppression]);
    
    // ğŸ¯ 3. æ–°å¢ä¸€ä¸ªuseEffectæ¥å®æ—¶æ›´æ–°å‰ç½®å¢ç›Š
    useEffect(() => {
        // ç¡®ä¿éŸ³é¢‘ç®¡çº¿å·²åˆå§‹åŒ–å¹¶ä¸”preampNodeå·²å­˜åœ¨
        if (isInitialized && preampNodeRef.current && audioContextRef.current) {
            console.log(`ğŸ”Š åº”ç”¨æ–°çš„å‰ç½®å¢ç›Šå€¼: ${settings.preamp}`);
            // ä½¿ç”¨setTargetAtTimeå¯ä»¥å¹³æ»‘åœ°æ”¹å˜éŸ³é‡ï¼Œé¿å…äº§ç”Ÿçˆ†éŸ³
            preampNodeRef.current.gain.setTargetAtTime(
                settings.preamp, 
                audioContextRef.current.currentTime, 
                0.02 // åœ¨0.02ç§’å†…å¹³æ»‘è¿‡æ¸¡åˆ°æ–°éŸ³é‡
            );
        }
    }, [settings.preamp, isInitialized]); // è¿™ä¸ªeffectåªåœ¨`settings.preamp`æˆ–`isInitialized`å˜åŒ–æ—¶è¿è¡Œ

    useEffect(() => {
        // ç¡®ä¿éŸ³é¢‘ç®¡çº¿å·²åˆå§‹åŒ–å¹¶ä¸”postampNodeå·²å­˜åœ¨
        if (isInitialized && postampNodeRef.current && audioContextRef.current) {
            console.log(`ğŸ”Š åº”ç”¨æ–°çš„åç½®å¢ç›Šå€¼: ${settings.postamp}`);
            postampNodeRef.current.gain.setTargetAtTime(
                settings.postamp, 
                audioContextRef.current.currentTime, 
                0.02 // å¹³æ»‘è¿‡æ¸¡
            );
        }
    }, [settings.postamp, isInitialized]); // åªåœ¨`settings.postamp`å˜åŒ–æ—¶è¿è¡Œ

    // ç›‘å¬æˆ¿é—´è¿æ¥çŠ¶æ€ï¼Œè‡ªåŠ¨åˆå§‹åŒ–
    useEffect(() => {
        if (!localParticipant || !room || room.state !== 'connected') {
            return;
        }

        if (isInitialized || isInitializingRef.current) {
            return;
        }

        console.log('ğŸ›ï¸ æˆ¿é—´å·²è¿æ¥ï¼Œå‡†å¤‡åˆå§‹åŒ–éŸ³é¢‘å¤„ç†');

        const timer = setTimeout(() => {
            initializeAudioProcessing(currentDeviceIdRef.current || 'default').catch(error => {
                console.error('âŒ éŸ³é¢‘å¤„ç†è‡ªåŠ¨åˆå§‹åŒ–å¤±è´¥:', error);
            });
        }, 1500);

        return () => clearTimeout(timer);
    }, [localParticipant, room, room?.state, isInitialized, initializeAudioProcessing]);

    // æ¸…ç†å‡½æ•°
    useEffect(() => {
        return () => {
            console.log('ğŸ§¹ æ¸…ç†éŸ³é¢‘å¤„ç†æ¨¡å—');
            
            // åœæ­¢å®æ—¶ç›‘æ§
            if (animationFrameRef.current) {
                cancelAnimationFrame(animationFrameRef.current);
                animationFrameRef.current = null;
            }
            
            // æ¸…ç† Web Audio API èŠ‚ç‚¹
            if (sourceNodeRef.current) {
                sourceNodeRef.current.disconnect();
                sourceNodeRef.current = null;
            }
            
            // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
            if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
                audioContextRef.current.close();
                audioContextRef.current = null;
            }
            
            // åœæ­¢åŸå§‹æµ
            if (originalStreamRef.current) {
                originalStreamRef.current.getTracks().forEach(track => track.stop());
                originalStreamRef.current = null;
            }
            
            // æ¸…ç†è½¨é“å¼•ç”¨
            if (processedTrackRef.current) {
                processedTrackRef.current = null;
            }
            
            // é‡ç½®æ‰€æœ‰èŠ‚ç‚¹å¼•ç”¨
            gateNodeRef.current = null;
            analyserNodeRef.current = null;
            destinationNodeRef.current = null;
            audioDataRef.current = null;
            
            setIsProcessingActive(false);
            setIsInitialized(false);
            isInitializingRef.current = false;
        };
    }, []);

    return {
        settings,
        updateSetting,
        isApplying,
        isProcessingActive,
        isInitialized,
        audioLevel,
        isVADActive
    };
}