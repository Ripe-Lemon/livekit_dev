// app/hooks/useAudioProcessing.ts
'use client';

import { useState, useEffect, useCallback, useRef } from 'react';
import { useLocalParticipant, useRoomContext } from '@livekit/components-react';
import { Track, LocalAudioTrack } from 'livekit-client';
import { MicVAD } from '@ricky0123/vad-web';

export interface AudioProcessingSettings {
    autoGainControl: boolean;
    noiseSuppression: boolean;
    echoCancellation: boolean;
    vadEnabled: boolean;
    vadPositiveSpeechThreshold: number; 
    vadNegativeSpeechThreshold: number;
    // VAD é™éŸ³å»¶è¿Ÿï¼Œå€¼è¶Šå¤§ï¼Œè¯­éŸ³ç»“æŸåç­‰å¾…æ—¶é—´è¶Šé•¿
    vadRedemptionFrames: number;
    sampleRate: number;
    channels: number;
}

export interface AudioProcessingControls {
    settings: AudioProcessingSettings;
    updateSetting: (key: keyof AudioProcessingSettings, value: boolean | number) => Promise<void>;
    isApplying: (key: keyof AudioProcessingSettings) => boolean;
    resetToDefaults: () => Promise<void>;
    isProcessingActive: boolean;
    isInitialized: boolean;
    audioLevel: number;
    isVADActive: boolean;
}

const DEFAULT_SETTINGS: Omit<AudioProcessingSettings, 'echoCancellation'> = {
    autoGainControl: true,
    noiseSuppression: true,
    vadEnabled: true,
    vadPositiveSpeechThreshold: 0.5, // å®˜æ–¹é»˜è®¤å€¼
    vadNegativeSpeechThreshold: 0.35,
    vadRedemptionFrames: 8,          // å®˜æ–¹é»˜è®¤å€¼
    sampleRate: 48000,
    channels: 1,
};

const STORAGE_KEY = 'livekit_audio_processing_settings';
type StoredSettings = Partial<AudioProcessingSettings & { microphoneThreshold?: number }>;

export function useAudioProcessing(): AudioProcessingControls {
    const { localParticipant } = useLocalParticipant();
    const room = useRoomContext();
    
    // çŠ¶æ€ç®¡ç†
    const [settings, setSettings] = useState<AudioProcessingSettings>(() => {
        if (typeof window === 'undefined') return { ...DEFAULT_SETTINGS, echoCancellation: false };
        try {
            const stored = localStorage.getItem(STORAGE_KEY);
            if (stored) {
                const parsed: StoredSettings = JSON.parse(stored);
                // ç§»é™¤å·²åºŸå¼ƒçš„ microphoneThreshold å±æ€§
                delete parsed.microphoneThreshold;
                return { ...DEFAULT_SETTINGS, echoCancellation: false, ...parsed };
            }
            return { ...DEFAULT_SETTINGS, echoCancellation: false };
        } catch {
            return { ...DEFAULT_SETTINGS, echoCancellation: false };
        }
    });
    
    const [applyingSettings, setApplyingSettings] = useState<Set<string>>(new Set());
    const [isProcessingActive, setIsProcessingActive] = useState(false);
    const [isInitialized, setIsInitialized] = useState(false);
    const [audioLevel, setAudioLevel] = useState(0);
    
    const [isVADActive, setIsVADActive] = useState(false);

    // Web Audio API èŠ‚ç‚¹å¼•ç”¨
    const audioContextRef = useRef<AudioContext | null>(null);
    const sourceNodeRef = useRef<MediaStreamAudioSourceNode | null>(null);
    const destinationNodeRef = useRef<MediaStreamAudioDestinationNode | null>(null);
    
    // éŸ³é¢‘å¤„ç†èŠ‚ç‚¹
    const gainNodeRef = useRef<GainNode | null>(null);
    const compressorNodeRef = useRef<DynamicsCompressorNode | null>(null);
    const highpassFilterRef = useRef<BiquadFilterNode | null>(null);
    const lowpassFilterRef = useRef<BiquadFilterNode | null>(null);
    const gateNodeRef = useRef<GainNode | null>(null);
    const vadRef = useRef<MicVAD | null>(null); // å¼•ç”¨ç±»å‹æ›´æ–°ä¸º MicVAD
    const analyserNodeRef = useRef<AnalyserNode | null>(null);
    
    // å…¶ä»–å¼•ç”¨
    const originalStreamRef = useRef<MediaStream | null>(null);
    const processedTrackRef = useRef<LocalAudioTrack | null>(null);
    const isInitializingRef = useRef<boolean>(false);
    const animationFrameRef = useRef<number | null>(null);
    const audioDataRef = useRef<Uint8Array | null>(null);

    // ä¿å­˜è®¾ç½®åˆ°æœ¬åœ°å­˜å‚¨
    const saveSettings = useCallback((newSettings: AudioProcessingSettings) => {
        if (typeof window !== 'undefined') {
            try {
                localStorage.setItem(STORAGE_KEY, JSON.stringify(newSettings));
            } catch (error) {
                console.warn('ä¿å­˜éŸ³é¢‘å¤„ç†è®¾ç½®å¤±è´¥:', error);
            }
        }
    }, []);

    // è·å–å½“å‰éŸ³é¢‘è®¾å¤‡ID
    const getCurrentAudioDeviceId = useCallback((): string => {
        // 1. ä¼˜å…ˆæ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ‰‹åŠ¨é€‰æ‹©äº†è®¾å¤‡
        try {
            const storedDevices = localStorage.getItem('livekit_selected_devices');
            if (storedDevices) {
                const parsed = JSON.parse(storedDevices);
                if (parsed.audioinput && parsed.audioinput !== 'default') {
                    console.log('ğŸ¤ ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„è®¾å¤‡:', parsed.audioinput);
                    return parsed.audioinput;
                }
            }
        } catch (error) {
            console.warn('è¯»å–å­˜å‚¨çš„è®¾å¤‡é€‰æ‹©å¤±è´¥:', error);
        }
        
        // 2. æ£€æŸ¥å½“å‰è½¨é“è®¾å¤‡
        if (processedTrackRef.current?.mediaStreamTrack) {
            const trackSettings = processedTrackRef.current.mediaStreamTrack.getSettings();
            if (trackSettings.deviceId) {
                console.log('ğŸ¤ ä½¿ç”¨å½“å‰è½¨é“è®¾å¤‡:', trackSettings.deviceId);
                return trackSettings.deviceId;
            }
        }
        
        // 3. ğŸ¯ é»˜è®¤è¿”å› 'default' è®¾å¤‡ï¼ˆç³»ç»Ÿé»˜è®¤éº¦å…‹é£ï¼‰
        console.log('ğŸ¤ ä½¿ç”¨ç³»ç»Ÿé»˜è®¤éº¦å…‹é£è®¾å¤‡');
        return 'default';
    }, []);

    const controlGate = useCallback((action: 'open' | 'close') => {
        // ä½¿ç”¨å‡½æ•°å¼æ›´æ–°ï¼Œé¿å…isVADActiveçš„é™ˆæ—§çŠ¶æ€é—®é¢˜
        if (action === 'open') {
            setIsVADActive(true);
        } else {
            setIsVADActive(false);
        }

        if (gateNodeRef.current && audioContextRef.current?.state === 'running') {
            const gateNode = gateNodeRef.current;
            const audioContext = audioContextRef.current;
            const targetGain = action === 'open' ? 1.0 : 0.0001;
            const delay = action === 'open' ? 0.1 : 0.5;
            gateNode.gain.exponentialRampToValueAtTime(targetGain, audioContext.currentTime + delay);
        }
    }, []);

    // ğŸ¯ 2. ä¿®æ­£ï¼šæ›´æ–° VAD åˆå§‹åŒ–å’Œå¯åŠ¨é€»è¾‘
    const initializeVAD = useCallback(async (stream: MediaStream) => {
        if (vadRef.current) {
            // å…ˆæš‚åœå¹¶é”€æ¯æ—§å®ä¾‹
            vadRef.current.destroy();
        }

        try {
            console.log('ğŸ¤ æ­£åœ¨åŠ è½½ VAD æ¨¡å‹å¹¶åº”ç”¨è®¾ç½®:', {
                positiveSpeechThreshold: settings.vadPositiveSpeechThreshold,
                redemptionFrames: settings.vadRedemptionFrames
            });
            
            // ä½¿ç”¨ MicVAD.new() å¹¶ç›´æ¥åœ¨æ„é€ å‡½æ•°ä¸­ä¼ å…¥ stream
            const vad = await MicVAD.new({
                // å…³é”®ï¼šåœ¨è¿™é‡Œä¼ å…¥æˆ‘ä»¬è‡ªå·±åˆ›å»ºçš„éŸ³é¢‘æµ
                stream: stream,

                // --- å›è°ƒå‡½æ•° ---
                onSpeechStart: () => {
                    console.log('VAD: æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹');
                    controlGate('open');
                },
                onSpeechEnd: (audio) => { // audio å‚æ•°æ˜¯å½•åˆ¶çš„éŸ³é¢‘æ•°æ®ï¼Œæˆ‘ä»¬ç”¨ä¸ä¸Šä½†å¯ä»¥æ¥æ”¶
                    console.log('VAD: æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ');
                    controlGate('close');
                },
                // å»ºè®®ï¼šæ·»åŠ å¯¹ VADMisfire çš„å¤„ç†ï¼Œç”¨äºè°ƒè¯•
                onVADMisfire: () => {
                    console.log('VAD Misfire: æ£€æµ‹åˆ°è¿‡çŸ­çš„è¯­éŸ³ç‰‡æ®µï¼Œå·²å¿½ç•¥');
                    controlGate('close');
                },
                ...settings,

                // å…¶ä»–å‚æ•°å¯ä¿æŒé»˜è®¤æˆ–æ ¹æ®éœ€è¦æš´éœ²
                minSpeechFrames: 3,            //
                preSpeechPadFrames: 1,         //
            });
            
            // å®ä¾‹åˆ›å»ºåç›´æ¥å¯åŠ¨ç›‘å¬
            vad.start();
            vadRef.current = vad;
            console.log('âœ… VAD æ¨¡å‹åŠ è½½å¹¶å¯åŠ¨æˆåŠŸ');
            
        } catch (error) {
            console.error('âŒ VAD åˆå§‹åŒ–å¤±è´¥:', error);
            // VADå¤±è´¥æ—¶ï¼Œé»˜è®¤æ‰“å¼€éŸ³é¢‘é—¨ï¼Œä»¥ä¿è¯é€šè¯å¯ç”¨æ€§
            controlGate('open');
        }

    }, [controlGate, settings]);

    // åˆå§‹åŒ– Web Audio API å¤„ç†é“¾
    const initializeAudioProcessingChain = useCallback(async () => {
        console.log('ğŸ›ï¸ åˆå§‹åŒ–éŸ³é¢‘å¤„ç†é“¾...');
        
        try {
            // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
            audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({
                sampleRate: settings.sampleRate,
                latencyHint: 'interactive'
            });

            const audioContext = audioContextRef.current;

            // åˆ›å»ºéŸ³é¢‘å¤„ç†èŠ‚ç‚¹é“¾
            gainNodeRef.current = audioContext.createGain();
            compressorNodeRef.current = audioContext.createDynamicsCompressor();
            highpassFilterRef.current = audioContext.createBiquadFilter();
            lowpassFilterRef.current = audioContext.createBiquadFilter();
            gateNodeRef.current = audioContext.createGain();
            analyserNodeRef.current = audioContext.createAnalyser();
            destinationNodeRef.current = audioContext.createMediaStreamDestination();

            gateNodeRef.current.gain.value = 0.0;

            // é…ç½®åˆ†æå™¨
            analyserNodeRef.current.fftSize = 256;
            analyserNodeRef.current.smoothingTimeConstant = 0.8;
            audioDataRef.current = new Uint8Array(analyserNodeRef.current.frequencyBinCount);

            // é…ç½®æ»¤æ³¢å™¨ï¼ˆå™ªå£°æŠ‘åˆ¶ï¼‰
            highpassFilterRef.current.type = 'highpass';
            highpassFilterRef.current.frequency.value = 85; // å»é™¤ä½é¢‘å™ªéŸ³
            highpassFilterRef.current.Q.value = 0.7;

            lowpassFilterRef.current.type = 'lowpass';
            lowpassFilterRef.current.frequency.value = 16000; // å»é™¤é«˜é¢‘å™ªéŸ³
            lowpassFilterRef.current.Q.value = 0.7;

            // é…ç½®å‹ç¼©å™¨ï¼ˆè‡ªåŠ¨å¢ç›Šæ§åˆ¶ï¼‰
            compressorNodeRef.current.threshold.value = -24;
            compressorNodeRef.current.knee.value = 30;
            compressorNodeRef.current.ratio.value = 12;
            compressorNodeRef.current.attack.value = 0.003;
            compressorNodeRef.current.release.value = 0.25;

            // é…ç½®å¢ç›ŠèŠ‚ç‚¹
            gainNodeRef.current.gain.value = 1.0;
            
            console.log('âœ… éŸ³é¢‘å¤„ç†é“¾èŠ‚ç‚¹åˆ›å»ºå®Œæˆ');
            return true;
        } catch (error) {
            console.error('âŒ éŸ³é¢‘å¤„ç†é“¾åˆå§‹åŒ–å¤±è´¥:', error);
            throw error;
        }
    }, [settings.sampleRate]);

    // è¿æ¥éŸ³é¢‘å¤„ç†é“¾
    const connectAudioChain = useCallback(() => {
        if (!sourceNodeRef.current || !destinationNodeRef.current) return;

        console.log('ğŸ”— è¿æ¥éŸ³é¢‘å¤„ç†é“¾...');

        // æ„å»ºå¤„ç†é“¾ï¼šè¾“å…¥ â†’ åˆ†æå™¨ â†’ å¢ç›Š â†’ é«˜é€šæ»¤æ³¢ â†’ ä½é€šæ»¤æ³¢ â†’ å‹ç¼©å™¨ â†’ å™ªå£°é—¨ â†’ è¾“å‡º
        sourceNodeRef.current.connect(analyserNodeRef.current!);
        analyserNodeRef.current!.connect(gainNodeRef.current!);
        gainNodeRef.current!.connect(highpassFilterRef.current!);
        highpassFilterRef.current!.connect(lowpassFilterRef.current!);
        lowpassFilterRef.current!.connect(compressorNodeRef.current!);
        compressorNodeRef.current!.connect(gateNodeRef.current!);
        gateNodeRef.current!.connect(destinationNodeRef.current);

        console.log('âœ… éŸ³é¢‘å¤„ç†é“¾å·²è¿æ¥');
    }, []);

    // ğŸ¯ å®æ—¶æ›´æ–°éŸ³é¢‘å¤„ç†å‚æ•°ï¼ˆä¸é‡å»ºè½¨é“ï¼‰
    const updateProcessingChain = useCallback(() => {
        if (!isInitialized) return;

        console.log('âš™ï¸ å®æ—¶æ›´æ–°éŸ³é¢‘å¤„ç†å‚æ•°:', settings);

        // 1. è‡ªåŠ¨å¢ç›Šæ§åˆ¶ï¼ˆé€šè¿‡å‹ç¼©å™¨å®ç°ï¼‰
        if (compressorNodeRef.current) {
            if (settings.autoGainControl) {
                compressorNodeRef.current.threshold.value = -24;
                compressorNodeRef.current.ratio.value = 12;
                compressorNodeRef.current.attack.value = 0.003;
                compressorNodeRef.current.release.value = 0.25;
                gainNodeRef.current!.gain.value = 1.2; // ç¨å¾®æå‡éŸ³é‡
            } else {
                compressorNodeRef.current.threshold.value = -50; // å‡ ä¹ä¸å‹ç¼©
                compressorNodeRef.current.ratio.value = 1;
                gainNodeRef.current!.gain.value = 1.0;
            }
        }

        // 2. å™ªå£°æŠ‘åˆ¶ï¼ˆé€šè¿‡æ»¤æ³¢å™¨å®ç°ï¼‰
        if (highpassFilterRef.current && lowpassFilterRef.current) {
            if (settings.noiseSuppression) {
                highpassFilterRef.current.frequency.value = 85; // è¿‡æ»¤ä½é¢‘å™ªéŸ³
                lowpassFilterRef.current.frequency.value = 8000; // è¿‡æ»¤é«˜é¢‘å™ªéŸ³
                highpassFilterRef.current.Q.value = 0.7;
                lowpassFilterRef.current.Q.value = 0.7;
            } else {
                highpassFilterRef.current.frequency.value = 20; // æœ€å°å€¼
                lowpassFilterRef.current.frequency.value = 20000; // æœ€å¤§å€¼
                highpassFilterRef.current.Q.value = 0.1;
                lowpassFilterRef.current.Q.value = 0.1;
            }
        }

        console.log('âœ… éŸ³é¢‘å¤„ç†å‚æ•°å·²å®æ—¶æ›´æ–°');
    }, [settings, isInitialized]);

    // å®æ—¶éŸ³é¢‘ç›‘æ§å’Œå™ªå£°é—¨æ§
    const startAudioMonitoring = useCallback(() => {
        if (!analyserNodeRef.current || !audioDataRef.current) return;
        const analyser = analyserNodeRef.current;
        const audioData = audioDataRef.current;
        
        const processFrame = () => {
            if (analyser) {
                analyser.getByteFrequencyData(audioData);
                let sum = 0;
                for (let i = 0; i < audioData.length; i++) {
                    sum += audioData[i] * audioData[i];
                }
                const rms = Math.sqrt(sum / audioData.length);
                // ç›´æ¥æ›´æ–°çŠ¶æ€ï¼Œè¿™æ˜¯æ€§èƒ½ç“¶é¢ˆçš„æ¥æºï¼Œä½†æˆ‘ä»¬éœ€è¦å®ƒ
                // UIå±‚çš„ä¼˜åŒ–å°†è§£å†³è¿™ä¸ªé—®é¢˜
                setAudioLevel(rms / 255);
            }
            animationFrameRef.current = requestAnimationFrame(processFrame);
        };
        processFrame();

        return () => {
            if (animationFrameRef.current) {
                cancelAnimationFrame(animationFrameRef.current);
            }
        };
    }, []);

    // ğŸ¯ æ–°å¢ï¼šç¡®ä¿ AudioContext å¤„äºè¿è¡ŒçŠ¶æ€çš„å‡½æ•°
    const ensureAudioContextRunning = useCallback(async () => {
        if (audioContextRef.current && audioContextRef.current.state === 'suspended') {
            console.log('AudioContext is suspended, attempting to resume...');
            try {
                await audioContextRef.current.resume();
                console.log('âœ… AudioContext resumed successfully.');
            } catch (e) {
                console.error('âŒ Failed to resume AudioContext:', e);
            }
        }
    }, []);

    // ğŸ¯ æ ¸å¿ƒï¼šåˆå§‹åŒ–éŸ³é¢‘å¤„ç†ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    const initializeAudioProcessing = useCallback(async () => {
        if (isInitializingRef.current || isInitialized || !localParticipant || !room) return;
        isInitializingRef.current = true;
        
        try {
            console.log('ğŸ›ï¸ å¼€å§‹åˆå§‹åŒ–è‡ªå®šä¹‰éŸ³é¢‘å¤„ç†ç³»ç»Ÿ...');
            // æ¸…ç†ç°æœ‰è½¨é“...
            const existingAudioPublications = Array.from(localParticipant.audioTrackPublications.values());
            for (const pub of existingAudioPublications) {
                if (pub.track) {
                    await localParticipant.unpublishTrack(pub.track);
                }
            }
            await new Promise(resolve => setTimeout(resolve, 200));

            // ğŸ¯ æ­¥éª¤2ï¼šåˆå§‹åŒ– Web Audio API å¤„ç†é“¾
            await initializeAudioProcessingChain();

            // ğŸ¯ æ­¥éª¤3ï¼šè·å–åŸå§‹éŸ³é¢‘æµï¼ˆä½¿ç”¨é»˜è®¤è®¾å¤‡ï¼Œç¦ç”¨æµè§ˆå™¨å¤„ç†ï¼‰
            const deviceId = getCurrentAudioDeviceId();
            const constraints: MediaStreamConstraints = {
                audio: {
                    // ğŸ¯ è®¾å¤‡é€‰æ‹©
                    deviceId: deviceId === 'default' ? undefined : { exact: deviceId },
                    
                    // ğŸ¯ å…³é”®ï¼šç¦ç”¨æ‰€æœ‰æµè§ˆå™¨åŸç”ŸéŸ³é¢‘å¤„ç†
                    echoCancellation: false,  // ç¦ç”¨å›å£°æŠ‘åˆ¶
                    noiseSuppression: false,  // ç¦ç”¨å™ªå£°æŠ‘åˆ¶  
                    autoGainControl: false,   // ç¦ç”¨è‡ªåŠ¨å¢ç›Š
                    
                    // éŸ³é¢‘è´¨é‡è®¾ç½®
                    sampleRate: { ideal: settings.sampleRate },
                    channelCount: { exact: 1 } // å¼ºåˆ¶å•å£°é“
                }
            };

            console.log('ğŸ¤ è·å–åŸå§‹éŸ³é¢‘æµï¼ˆå·²ç¦ç”¨æµè§ˆå™¨å¤„ç†ï¼‰:', constraints);
            const originalStream = await navigator.mediaDevices.getUserMedia(constraints);
            originalStreamRef.current = originalStream;
            await ensureAudioContextRunning();

            // ğŸ¯ æ­¥éª¤4ï¼šè¿æ¥åˆ° Web Audio API å¤„ç†é“¾
            sourceNodeRef.current = audioContextRef.current!.createMediaStreamSource(originalStream);
            connectAudioChain();

            if (settings.vadEnabled) {
                await initializeVAD(originalStream);
            } else {
                // å¦‚æœVADè¢«ç¦ç”¨ï¼Œåˆ™æ‰‹åŠ¨æ‰“å¼€é—¨
                controlGate('open');
            }
            
            // å‘å¸ƒè½¨é“
            const processedStream = destinationNodeRef.current!.stream;
            const processedAudioTrack = processedStream.getAudioTracks()[0];
            processedTrackRef.current = new LocalAudioTrack(processedAudioTrack, undefined, false);
            await localParticipant.publishTrack(processedTrackRef.current, { name: 'custom-microphone-vad', source: Track.Source.Microphone, stopMicTrackOnMute: false });

            // æ›´æ–°å’Œå¯åŠ¨ç›‘æ§
            updateProcessingChain();
            startAudioMonitoring();

            setIsProcessingActive(true);
            setIsInitialized(true);
            console.log('âœ… è‡ªå®šä¹‰éŸ³é¢‘å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ');
        } catch (error) {
            console.error('âŒ è‡ªå®šä¹‰éŸ³é¢‘å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥:', error);
            setIsProcessingActive(false);
            throw error;
        } finally {
            isInitializingRef.current = false;
        }
    }, [
        localParticipant, 
        room, 
        isInitialized, 
        settings,
        initializeVAD, 
        controlGate,
        initializeAudioProcessingChain,
        getCurrentAudioDeviceId,
        connectAudioChain,
        updateProcessingChain,
        startAudioMonitoring,
        ensureAudioContextRunning
    ]);

    // ğŸ¯ æ›´æ–°å•ä¸ªè®¾ç½®ï¼ˆåªæ›´æ–°å¤„ç†å‚æ•°ï¼Œä¸é‡å»ºè½¨é“ï¼‰
    const updateSetting = useCallback(async (key: keyof AudioProcessingSettings, value: boolean | number): Promise<void> => {
        setSettings(prevSettings => {
            const newSettings = { ...prevSettings, [key]: value };
            saveSettings(newSettings);
            return newSettings;
        });
    }, []);

    // ä½¿ç”¨ä¸“é—¨çš„useEffectæ¥å“åº”è®¾ç½®å˜åŒ–
    useEffect(() => {
        if (!isInitialized) return;

        const handleSettingsChange = async () => {
            // å¦‚æœVADè¢«å¯ç”¨ï¼Œåˆ™æ ¹æ®æœ€æ–°è®¾ç½®é‡æ–°åˆå§‹åŒ–
            if (settings.vadEnabled) {
                if (originalStreamRef.current) {
                    console.log('VAD settings changed, re-initializing VAD...');
                    await initializeVAD(originalStreamRef.current);
                }
            } else { // å¦‚æœVADè¢«ç¦ç”¨
                if (vadRef.current) {
                    vadRef.current.destroy();
                    vadRef.current = null;
                }
                controlGate('open'); // æ‰‹åŠ¨æ‰“å¼€é—¨
                console.log('VAD is disabled, gate opened.');
            }
        };
        
        handleSettingsChange();

    }, [settings, isInitialized, initializeVAD, controlGate]); // ç›‘å¬æ•´ä¸ªsettingså¯¹è±¡çš„å˜åŒ–

    // æ£€æŸ¥æ˜¯å¦æ­£åœ¨åº”ç”¨è®¾ç½®
    const isApplying = useCallback((key: keyof AudioProcessingSettings) => {
        return applyingSettings.has(key);
    }, [applyingSettings]);

    // é‡ç½®ä¸ºé»˜è®¤è®¾ç½®
    const resetToDefaults = useCallback(async () => {
        console.log('ğŸ”„ é‡ç½®éŸ³é¢‘å¤„ç†è®¾ç½®ä¸ºé»˜è®¤å€¼');
        
        try {
            const defaultWithEcho = { ...DEFAULT_SETTINGS, echoCancellation: false };
            setSettings(defaultWithEcho);
            saveSettings(defaultWithEcho);
            
            if (isInitialized) {
                updateProcessingChain();
                if (DEFAULT_SETTINGS.vadEnabled) {
                    if(originalStreamRef.current) await initializeVAD(originalStreamRef.current);
                } else {
                    if(vadRef.current) vadRef.current.destroy();
                    controlGate('open');
                }
            }
            
            console.log('âœ… å·²é‡ç½®ä¸ºé»˜è®¤è®¾ç½®');
        } catch (error) {
            console.error('âŒ é‡ç½®è®¾ç½®å¤±è´¥:', error);
            throw error;
        }
    }, [saveSettings, isInitialized, updateProcessingChain]);

    // ç›‘å¬æˆ¿é—´è¿æ¥çŠ¶æ€ï¼Œè‡ªåŠ¨åˆå§‹åŒ–
    useEffect(() => {
        if (!localParticipant || !room || room.state !== 'connected') {
            return;
        }

        if (isInitialized || isInitializingRef.current) {
            return;
        }

        console.log('ğŸ›ï¸ æˆ¿é—´å·²è¿æ¥ï¼Œå‡†å¤‡åˆå§‹åŒ–éŸ³é¢‘å¤„ç†');

        const timer = setTimeout(() => {
            initializeAudioProcessing().catch(error => {
                console.error('âŒ éŸ³é¢‘å¤„ç†è‡ªåŠ¨åˆå§‹åŒ–å¤±è´¥:', error);
            });
        }, 1500);

        return () => clearTimeout(timer);
    }, [localParticipant, room, room?.state, isInitialized, initializeAudioProcessing]);

    // æ¸…ç†å‡½æ•°
    useEffect(() => {
        return () => {
            console.log('ğŸ§¹ æ¸…ç†éŸ³é¢‘å¤„ç†æ¨¡å—');
            
            if (vadRef.current) {
                vadRef.current.destroy(); // destroy ä¼šå¤„ç†æš‚åœå’Œèµ„æºé‡Šæ”¾
                vadRef.current = null;
            }
            
            // åœæ­¢å®æ—¶ç›‘æ§
            if (animationFrameRef.current) {
                cancelAnimationFrame(animationFrameRef.current);
                animationFrameRef.current = null;
            }
            
            // æ¸…ç† Web Audio API èŠ‚ç‚¹
            if (sourceNodeRef.current) {
                sourceNodeRef.current.disconnect();
                sourceNodeRef.current = null;
            }
            
            // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
            if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
                audioContextRef.current.close();
                audioContextRef.current = null;
            }
            
            // åœæ­¢åŸå§‹æµ
            if (originalStreamRef.current) {
                originalStreamRef.current.getTracks().forEach(track => track.stop());
                originalStreamRef.current = null;
            }
            
            // æ¸…ç†è½¨é“å¼•ç”¨
            if (processedTrackRef.current) {
                processedTrackRef.current = null;
            }
            
            // é‡ç½®æ‰€æœ‰èŠ‚ç‚¹å¼•ç”¨
            gainNodeRef.current = null;
            compressorNodeRef.current = null;
            highpassFilterRef.current = null;
            lowpassFilterRef.current = null;
            gateNodeRef.current = null;
            analyserNodeRef.current = null;
            destinationNodeRef.current = null;
            audioDataRef.current = null;
            
            setIsProcessingActive(false);
            setIsInitialized(false);
            isInitializingRef.current = false;
        };
    }, []);

    return {
        settings,
        updateSetting,
        isApplying,
        resetToDefaults,
        isProcessingActive,
        isInitialized,
        audioLevel,
        isVADActive
    };
}