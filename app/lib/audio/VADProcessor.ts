'use client';

export interface VADResult {
    probability: number; // 0-1 èŒƒå›´ï¼Œè¯­éŸ³æ£€æµ‹æ¦‚ç‡
    isSpeaking: boolean; // æ˜¯å¦æ­£åœ¨è¯´è¯
    volume: number; // éŸ³é‡çº§åˆ« 0-1
}

export interface VADConfig {
    threshold: number; // è¯­éŸ³æ£€æµ‹é˜ˆå€¼ 0-1
    smoothingFactor: number; // å¹³æ»‘å› å­ 0-1
    minSpeechFrames: number; // æœ€å°è¯­éŸ³å¸§æ•°
    minSilenceFrames: number; // æœ€å°é™éŸ³å¸§æ•°
    analyzeWindow: number; // åˆ†æçª—å£å¤§å°(æ¯«ç§’)
}

const DEFAULT_VAD_CONFIG: VADConfig = {
    threshold: 0.3,
    smoothingFactor: 0.8,
    minSpeechFrames: 3,
    minSilenceFrames: 10,
    analyzeWindow: 30 // 30ms
};

export class VADProcessor {
    private audioContext: AudioContext | null = null;
    private analyserNode: AnalyserNode | null = null;
    private microphoneSource: MediaStreamAudioSourceNode | null = null;
    private isActive = false;
    private config: VADConfig;
    private dataArray: Uint8Array | null = null;
    private freqDataArray: Uint8Array | null = null;
    
    // VAD çŠ¶æ€
    private currentVolume = 0;
    private smoothedVolume = 0;
    private speechFrameCount = 0;
    private silenceFrameCount = 0;
    private isSpeaking = false;
    private lastAnalysisTime = 0;
    
    // é¢‘è°±åˆ†æç›¸å…³
    private speechFreqBands = {
        low: { min: 85, max: 255 },      // 85-255 Hz (åŸºé¢‘)
        mid: { min: 255, max: 2000 },    // 255-2000 Hz (ä¸»è¦è¯­éŸ³é¢‘æ®µ)
        high: { min: 2000, max: 4000 }   // 2000-4000 Hz (è¯­éŸ³æ¸…æ™°åº¦)
    };

    // å†å²æ•°æ®ç”¨äºæ›´æ™ºèƒ½çš„æ£€æµ‹
    private volumeHistory: number[] = [];
    private readonly historySize = 10;

    // å›è°ƒå‡½æ•°
    private onVADUpdate: ((result: VADResult) => void) | null = null;

    constructor(config: Partial<VADConfig> = {}) {
        this.config = { ...DEFAULT_VAD_CONFIG, ...config };
        this.initializeAudioContext();
    }

    private initializeAudioContext() {
        try {
            this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
            console.log('ğŸ¤ VAD AudioContext å·²åˆå§‹åŒ–');
        } catch (error) {
            console.error('âŒ VAD AudioContext åˆå§‹åŒ–å¤±è´¥:', error);
        }
    }

    async connectToMicrophone(stream: MediaStream): Promise<void> {
        if (!this.audioContext) {
            throw new Error('AudioContext æœªåˆå§‹åŒ–');
        }

        try {
            // æ¢å¤ AudioContextï¼ˆå¦‚æœè¢«æš‚åœï¼‰
            if (this.audioContext.state === 'suspended') {
                await this.audioContext.resume();
            }

            // åˆ›å»ºéŸ³é¢‘åˆ†æèŠ‚ç‚¹
            this.analyserNode = this.audioContext.createAnalyser();
            this.analyserNode.fftSize = 2048;
            this.analyserNode.smoothingTimeConstant = 0.8;
            this.analyserNode.minDecibels = -90;
            this.analyserNode.maxDecibels = -10;

            // è¿æ¥éº¦å…‹é£è¾“å…¥
            this.microphoneSource = this.audioContext.createMediaStreamSource(stream);
            this.microphoneSource.connect(this.analyserNode);

            // åˆå§‹åŒ–æ•°æ®æ•°ç»„
            const bufferLength = this.analyserNode.frequencyBinCount;
            this.dataArray = new Uint8Array(bufferLength);
            this.freqDataArray = new Uint8Array(bufferLength);

            console.log('âœ… VAD å·²è¿æ¥åˆ°éº¦å…‹é£', {
                fftSize: this.analyserNode.fftSize,
                bufferLength,
                sampleRate: this.audioContext.sampleRate
            });

            this.startAnalysis();
        } catch (error) {
            console.error('âŒ VAD è¿æ¥éº¦å…‹é£å¤±è´¥:', error);
            throw error;
        }
    }

    private startAnalysis() {
        if (this.isActive) return;
        
        this.isActive = true;
        console.log('ğŸ” VAD åˆ†æå·²å¼€å§‹');
        
        const analyze = () => {
            if (!this.isActive || !this.analyserNode || !this.dataArray || !this.freqDataArray) {
                return;
            }

            const now = Date.now();
            if (now - this.lastAnalysisTime < this.config.analyzeWindow) {
                requestAnimationFrame(analyze);
                return;
            }
            this.lastAnalysisTime = now;

            // è·å–éŸ³é¢‘æ•°æ®
            this.analyserNode.getByteTimeDomainData(this.dataArray);
            this.analyserNode.getByteFrequencyData(this.freqDataArray);

            // åˆ†æéŸ³é¢‘
            const result = this.analyzeAudio();
            
            // è§¦å‘å›è°ƒ
            if (this.onVADUpdate) {
                this.onVADUpdate(result);
            }

            requestAnimationFrame(analyze);
        };

        requestAnimationFrame(analyze);
    }

    private analyzeAudio(): VADResult {
        if (!this.dataArray || !this.freqDataArray) {
            return { probability: 0, isSpeaking: false, volume: 0 };
        }

        // 1. è®¡ç®—éŸ³é‡ (RMS)
        const volume = this.calculateVolume();
        this.currentVolume = volume;

        // 2. å¹³æ»‘éŸ³é‡
        this.smoothedVolume = this.smoothedVolume * this.config.smoothingFactor + 
                             volume * (1 - this.config.smoothingFactor);

        // 3. é¢‘è°±åˆ†æ
        const spectralFeatures = this.analyzeSpectrum();

        // 4. è¯­éŸ³æ¦‚ç‡è®¡ç®—
        const probability = this.calculateSpeechProbability(this.smoothedVolume, spectralFeatures);

        // 5. çŠ¶æ€æœºé€»è¾‘
        const wasSpeaking = this.isSpeaking;
        this.updateSpeechState(probability);

        // 6. æ›´æ–°å†å²æ•°æ®
        this.updateHistory(volume);

        // 7. è°ƒè¯•æ—¥å¿—ï¼ˆé™åˆ¶é¢‘ç‡ï¼‰
        if (Math.random() < 0.01) { // 1% æ¦‚ç‡è¾“å‡ºè°ƒè¯•ä¿¡æ¯
            console.log('ğŸ” VAD åˆ†æç»“æœ:', {
                volume: volume.toFixed(3),
                smoothed: this.smoothedVolume.toFixed(3),
                probability: probability.toFixed(3),
                isSpeaking: this.isSpeaking,
                speechFrames: this.speechFrameCount,
                silenceFrames: this.silenceFrameCount,
                spectral: spectralFeatures
            });
        }

        return {
            probability,
            isSpeaking: this.isSpeaking,
            volume: this.smoothedVolume
        };
    }

    private calculateVolume(): number {
        if (!this.dataArray) return 0;

        let sum = 0;
        for (let i = 0; i < this.dataArray.length; i++) {
            const amplitude = (this.dataArray[i] - 128) / 128;
            sum += amplitude * amplitude;
        }
        
        const rms = Math.sqrt(sum / this.dataArray.length);
        return Math.min(rms * 2, 1); // æ”¾å¤§å¹¶é™åˆ¶åœ¨0-1èŒƒå›´
    }

    private analyzeSpectrum(): { speechEnergy: number; totalEnergy: number; spectralCentroid: number } {
        if (!this.freqDataArray || !this.audioContext) {
            return { speechEnergy: 0, totalEnergy: 0, spectralCentroid: 0 };
        }

        const nyquist = this.audioContext.sampleRate / 2;
        const binWidth = nyquist / this.freqDataArray.length;

        let totalEnergy = 0;
        let speechEnergy = 0;
        let weightedFreqSum = 0;
        let magnitudeSum = 0;

        for (let i = 0; i < this.freqDataArray.length; i++) {
            const frequency = i * binWidth;
            const magnitude = this.freqDataArray[i] / 255; // å½’ä¸€åŒ–åˆ°0-1
            
            totalEnergy += magnitude;
            magnitudeSum += magnitude;
            weightedFreqSum += frequency * magnitude;

            // è¯­éŸ³é¢‘æ®µèƒ½é‡ (85Hz - 4000Hz)
            if (frequency >= 85 && frequency <= 4000) {
                speechEnergy += magnitude;
            }
        }

        const spectralCentroid = magnitudeSum > 0 ? weightedFreqSum / magnitudeSum : 0;

        return {
            speechEnergy: speechEnergy / this.freqDataArray.length,
            totalEnergy: totalEnergy / this.freqDataArray.length,
            spectralCentroid
        };
    }

    private calculateSpeechProbability(volume: number, spectral: any): number {
        // å¤šç»´åº¦è¯­éŸ³æ¦‚ç‡è®¡ç®—
        
        // 1. éŸ³é‡æ¦‚ç‡ (Så‹æ›²çº¿)
        const volumeProb = 1 / (1 + Math.exp(-10 * (volume - this.config.threshold)));

        // 2. é¢‘è°±æ¦‚ç‡ (è¯­éŸ³é¢‘æ®µèƒ½é‡æ¯”ä¾‹)
        const spectralProb = spectral.totalEnergy > 0 ? 
            Math.min(spectral.speechEnergy / spectral.totalEnergy * 2, 1) : 0;

        // 3. é¢‘è°±é‡å¿ƒæ¦‚ç‡ (è¯­éŸ³é€šå¸¸åœ¨500-2000Hz)
        const centroidProb = spectral.spectralCentroid > 500 && spectral.spectralCentroid < 2000 ? 
            1 - Math.abs(spectral.spectralCentroid - 1250) / 1250 : 0;

        // 4. å†å²è¶‹åŠ¿æ¦‚ç‡
        const trendProb = this.calculateTrendProbability();

        // åŠ æƒç»„åˆ
        const probability = 
            volumeProb * 0.4 +      // éŸ³é‡æƒé‡æœ€é«˜
            spectralProb * 0.3 +    // é¢‘è°±èƒ½é‡æ¬¡ä¹‹
            centroidProb * 0.2 +    // é¢‘è°±é‡å¿ƒ
            trendProb * 0.1;        // å†å²è¶‹åŠ¿

        return Math.max(0, Math.min(1, probability));
    }

    private calculateTrendProbability(): number {
        if (this.volumeHistory.length < 3) return 0.5;

        const recent = this.volumeHistory.slice(-3);
        const avg = recent.reduce((a, b) => a + b, 0) / recent.length;
        const current = this.currentVolume;

        // å¦‚æœå½“å‰éŸ³é‡æ˜¾è‘—é«˜äºæœ€è¿‘å¹³å‡å€¼ï¼Œå¢åŠ è¯­éŸ³æ¦‚ç‡
        if (current > avg * 1.2) {
            return 0.8;
        } else if (current < avg * 0.8) {
            return 0.2;
        }
        return 0.5;
    }

    private updateSpeechState(probability: number) {
        if (probability > this.config.threshold) {
            this.speechFrameCount++;
            this.silenceFrameCount = 0;
            
            if (!this.isSpeaking && this.speechFrameCount >= this.config.minSpeechFrames) {
                this.isSpeaking = true;
                console.log('ğŸ—£ï¸ æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹');
            }
        } else {
            this.silenceFrameCount++;
            this.speechFrameCount = 0;
            
            if (this.isSpeaking && this.silenceFrameCount >= this.config.minSilenceFrames) {
                this.isSpeaking = false;
                console.log('ğŸ¤« æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ');
            }
        }
    }

    private updateHistory(volume: number) {
        this.volumeHistory.push(volume);
        if (this.volumeHistory.length > this.historySize) {
            this.volumeHistory.shift();
        }
    }

    updateConfig(newConfig: Partial<VADConfig>) {
        this.config = { ...this.config, ...newConfig };
        console.log('âš™ï¸ VAD é…ç½®å·²æ›´æ–°:', this.config);
    }

    setVADCallback(callback: (result: VADResult) => void) {
        this.onVADUpdate = callback;
    }

    stopAnalysis() {
        this.isActive = false;
        console.log('â¹ï¸ VAD åˆ†æå·²åœæ­¢');
    }

    disconnect() {
        this.stopAnalysis();
        
        if (this.microphoneSource) {
            this.microphoneSource.disconnect();
            this.microphoneSource = null;
        }
        
        if (this.analyserNode) {
            this.analyserNode.disconnect();
            this.analyserNode = null;
        }
        
        this.dataArray = null;
        this.freqDataArray = null;
        this.onVADUpdate = null;
        
        console.log('ğŸ”Œ VAD å·²æ–­å¼€è¿æ¥');
    }

    dispose() {
        this.disconnect();
        
        if (this.audioContext && this.audioContext.state !== 'closed') {
            this.audioContext.close();
            this.audioContext = null;
        }
        
        console.log('ğŸ—‘ï¸ VAD å·²é”€æ¯');
    }

    // è·å–å½“å‰çŠ¶æ€ç”¨äºè°ƒè¯•
    getDebugInfo() {
        return {
            isActive: this.isActive,
            config: this.config,
            currentVolume: this.currentVolume,
            smoothedVolume: this.smoothedVolume,
            isSpeaking: this.isSpeaking,
            speechFrameCount: this.speechFrameCount,
            silenceFrameCount: this.silenceFrameCount,
            volumeHistory: [...this.volumeHistory],
            audioContextState: this.audioContext?.state
        };
    }
}