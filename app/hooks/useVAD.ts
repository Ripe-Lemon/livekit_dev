import { VADProcessor, VADAudioGateway, VADResult, VADConfig } from '../lib/audio/VADProcessor';

export interface VADHookResult {
    vadResult: VADResult | null;
    isActive: boolean;
    startVAD: () => Promise<void>;
    stopVAD: () => void;
    updateThreshold: (threshold: number) => void;
    vadProcessor: VADProcessor | null;
    // æ–°å¢ï¼šç½‘å…³ç›¸å…³çŠ¶æ€
    isGatewayControlling: boolean;
    gatewayState: any;
}

export function useVAD(initialConfig?: Partial<VADConfig>): VADHookResult {
    const { localParticipant } = useLocalParticipant();
    const [vadResult, setVADResult] = useState<VADResult | null>(null);
    const [isActive, setIsActive] = useState(false);
    const [isGatewayControlling, setIsGatewayControlling] = useState(false);
    const [gatewayState, setGatewayState] = useState<any>(null);
    
    const vadProcessorRef = useRef<VADProcessor | null>(null);
    const audioGatewayRef = useRef<VADAudioGateway | null>(null);
    const analysisStreamRef = useRef<MediaStream | null>(null); // åˆ†æç”¨éŸ³é¢‘æµ
    const publishStreamRef = useRef<MediaStream | null>(null);  // å‘å¸ƒç”¨éŸ³é¢‘æµ

    // åˆå§‹åŒ–VADå¤„ç†å™¨å’ŒéŸ³é¢‘ç½‘å…³
    const initializeVAD = useCallback(() => {
        if (vadProcessorRef.current) {
            vadProcessorRef.current.dispose();
        }
        if (audioGatewayRef.current) {
            audioGatewayRef.current.dispose();
        }

        vadProcessorRef.current = new VADProcessor(initialConfig);
        audioGatewayRef.current = new VADAudioGateway();
        
        // è®¾ç½®VADç»“æœå›è°ƒ
        vadProcessorRef.current.setVADCallback((result: VADResult) => {
            setVADResult(result);
        });

        // è®¾ç½®è¯­éŸ³çŠ¶æ€å˜åŒ–å›è°ƒï¼Œæ§åˆ¶éŸ³é¢‘ç½‘å…³
        vadProcessorRef.current.setSpeechStateChangeCallback((isSpeaking: boolean) => {
            if (audioGatewayRef.current) {
                audioGatewayRef.current.setTransmitting(isSpeaking);
            }
        });

        // è®¾ç½®ç½‘å…³çŠ¶æ€å›è°ƒ
        audioGatewayRef.current.setStateChangeCallback((state) => {
            setGatewayState(state);
        });

        console.log('ğŸ¤ VAD å¤„ç†å™¨å’ŒéŸ³é¢‘ç½‘å…³å·²åˆå§‹åŒ–');
    }, [initialConfig]);

    // åˆ›å»ºåŸå§‹éŸ³é¢‘æµç”¨äºåˆ†æå’Œå¤„ç†
    const createDualAudioStreams = useCallback(async (): Promise<{
        analysisStream: MediaStream;
        publishStream: MediaStream;
    } | null> => {
        if (!localParticipant || !audioGatewayRef.current) {
            console.warn('æœ¬åœ°å‚ä¸è€…æˆ–éŸ³é¢‘ç½‘å…³ä¸å­˜åœ¨');
            return null;
        }

        try {
            // 1. é¦–å…ˆå°è¯•è·å–åŸå§‹è®¾å¤‡éŸ³é¢‘æµ
            console.log('ğŸ¤ åˆ›å»ºåŸå§‹éŸ³é¢‘æµç”¨äºVADåˆ†æå’Œå¤„ç†...');
            const originalStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: false,  // VADéœ€è¦åŸå§‹éŸ³é¢‘
                    noiseSuppression: false,  // ä¸è¦é™å™ªï¼ŒVADéœ€è¦åŸå§‹ä¿¡å·
                    autoGainControl: false,   // ä¸è¦è‡ªåŠ¨å¢ç›Š
                    sampleRate: 48000,
                    channelCount: 1
                }
            });

            // 2. åˆ†ææµï¼šç›´æ¥ä½¿ç”¨åŸå§‹æµ
            const analysisStream = originalStream;

            // 3. å‘å¸ƒæµï¼šé€šè¿‡éŸ³é¢‘ç½‘å…³å¤„ç†åŸå§‹æµ
            const publishStream = await audioGatewayRef.current.connectToStream(originalStream);
            if (!publishStream) {
                throw new Error('æ— æ³•åˆ›å»ºå¤„ç†åçš„å‘å¸ƒæµ');
            }

            console.log('âœ… åŒè½¨é“éŸ³é¢‘æµåˆ›å»ºæˆåŠŸ');
            console.log('ğŸ“Š åˆ†ææµè½¨é“æ•°:', analysisStream.getAudioTracks().length);
            console.log('ğŸ“Š å‘å¸ƒæµè½¨é“æ•°:', publishStream.getAudioTracks().length);

            return {
                analysisStream,
                publishStream
            };

        } catch (error) {
            console.error('âŒ åˆ›å»ºåŒè½¨é“éŸ³é¢‘æµå¤±è´¥:', error);
            return null;
        }
    }, [localParticipant]);

    // å¯åŠ¨VADå’ŒéŸ³é¢‘ç½‘å…³
    const startVAD = useCallback(async () => {
        if (isActive || !vadProcessorRef.current || !audioGatewayRef.current) {
            console.log('VAD å·²ç»æ´»è·ƒæˆ–ç»„ä»¶æœªåˆå§‹åŒ–');
            return;
        }

        try {
            console.log('ğŸ” å¯åŠ¨VADåŒè½¨é“ç³»ç»Ÿ...');
            
            // åˆ›å»ºåŒè½¨é“éŸ³é¢‘æµ
            const streams = await createDualAudioStreams();
            if (!streams) {
                throw new Error('æ— æ³•åˆ›å»ºåŒè½¨é“éŸ³é¢‘æµ');
            }

            const { analysisStream, publishStream } = streams;

            // éªŒè¯åˆ†ææµçŠ¶æ€
            const analysisTrack = analysisStream.getAudioTracks()[0];
            if (!analysisTrack || analysisTrack.readyState !== 'live') {
                throw new Error(`åˆ†æéŸ³é¢‘è½¨é“çŠ¶æ€æ— æ•ˆ: ${analysisTrack?.readyState}`);
            }

            // éªŒè¯å‘å¸ƒæµçŠ¶æ€
            const publishTrack = publishStream.getAudioTracks()[0];
            if (!publishTrack || publishTrack.readyState !== 'live') {
                throw new Error(`å‘å¸ƒéŸ³é¢‘è½¨é“çŠ¶æ€æ— æ•ˆ: ${publishTrack?.readyState}`);
            }

            console.log('ğŸ” éŸ³é¢‘è½¨é“è¯¦æƒ…:', {
                analysis: {
                    label: analysisTrack.label,
                    readyState: analysisTrack.readyState,
                    settings: analysisTrack.getSettings()
                },
                publish: {
                    label: publishTrack.label,
                    readyState: publishTrack.readyState,
                    settings: publishTrack.getSettings()
                }
            });

            // è¿æ¥VADåˆ°åˆ†ææµ
            await vadProcessorRef.current.connectToMicrophone(analysisStream);
            analysisStreamRef.current = analysisStream;

            // åœæ­¢å¹¶æ›¿æ¢LiveKitçš„ç°æœ‰éŸ³é¢‘è½¨é“
            const existingAudioPublication = localParticipant.getTrackPublication(Track.Source.Microphone);
            if (existingAudioPublication?.track) {
                console.log('ğŸ›‘ åœæ­¢ç°æœ‰LiveKitéŸ³é¢‘è½¨é“');
                existingAudioPublication.track.stop();
                await localParticipant.unpublishTrack(existingAudioPublication.track);
                
                // ç­‰å¾…è½¨é“æ¸…ç†
                await new Promise(resolve => setTimeout(resolve, 300));
            }

            // å‘å¸ƒå¤„ç†åçš„éŸ³é¢‘è½¨é“åˆ°LiveKitï¼ˆæ ‡è®°ä¸ºmicrophoneï¼‰
            console.log('ğŸ“¤ å‘å¸ƒVADå¤„ç†åçš„éŸ³é¢‘è½¨é“...');
            await localParticipant.publishTrack(publishTrack, {
                source: Track.Source.Microphone,
                name: 'microphone'
            });

            publishStreamRef.current = publishStream;
            setIsActive(true);
            setIsGatewayControlling(true);
            
            console.log('âœ… VADåŒè½¨é“ç³»ç»Ÿå·²å¯åŠ¨å¹¶è¿æ¥');
            console.log('ğŸ¯ åˆ†æè½¨é“ç”¨äºVADæ£€æµ‹ï¼Œå‘å¸ƒè½¨é“ç”¨äºæœåŠ¡å™¨ä¼ è¾“');

        } catch (error) {
            console.error('âŒ å¯åŠ¨VADåŒè½¨é“ç³»ç»Ÿå¤±è´¥:', error);
            setIsActive(false);
            setIsGatewayControlling(false);
            
            // æ¸…ç†å¤±è´¥çš„æµ
            if (analysisStreamRef.current) {
                analysisStreamRef.current.getTracks().forEach(track => track.stop());
                analysisStreamRef.current = null;
            }
            if (publishStreamRef.current) {
                publishStreamRef.current.getTracks().forEach(track => track.stop());
                publishStreamRef.current = null;
            }
            
            throw error;
        }
    }, [isActive, createDualAudioStreams, localParticipant]);

    // åœæ­¢VADå’ŒéŸ³é¢‘ç½‘å…³
    const stopVAD = useCallback(() => {
        if (!isActive) return;

        console.log('â¹ï¸ åœæ­¢VADåŒè½¨é“ç³»ç»Ÿ...');
        
        if (vadProcessorRef.current) {
            vadProcessorRef.current.stopAnalysis();
        }

        // æ¸…ç†åˆ†æéŸ³é¢‘æµ
        if (analysisStreamRef.current) {
            analysisStreamRef.current.getTracks().forEach(track => {
                console.log(`ğŸ›‘ åœæ­¢åˆ†æéŸ³é¢‘è½¨é“: ${track.label}`);
                track.stop();
            });
            analysisStreamRef.current = null;
        }

        // æ¸…ç†å‘å¸ƒéŸ³é¢‘æµï¼ˆæ³¨æ„ï¼šä¸è¦åœæ­¢å·²å‘å¸ƒåˆ°LiveKitçš„è½¨é“ï¼‰
        if (publishStreamRef.current) {
            console.log('ğŸ“¤ å‘å¸ƒæµå°†ç”±LiveKitç®¡ç†ï¼Œä¸æ‰‹åŠ¨åœæ­¢');
            publishStreamRef.current = null;
        }

        setIsActive(false);
        setIsGatewayControlling(false);
        setVADResult(null);
        setGatewayState(null);
        console.log('âœ… VADåŒè½¨é“ç³»ç»Ÿå·²åœæ­¢');
    }, [isActive]);

    // æ›´æ–°é˜ˆå€¼
    const updateThreshold = useCallback((threshold: number) => {
        if (vadProcessorRef.current) {
            vadProcessorRef.current.updateConfig({ threshold });
            console.log(`âš™ï¸ VAD é˜ˆå€¼å·²æ›´æ–°ä¸º: ${threshold}`);
        }
    }, []);

    // ç›‘å¬æœ¬åœ°å‚ä¸è€…å˜åŒ–
    useEffect(() => {
        if (localParticipant && !vadProcessorRef.current) {
            console.log('ğŸ¤ æœ¬åœ°å‚ä¸è€…å°±ç»ªï¼Œåˆå§‹åŒ–VADåŒè½¨é“ç³»ç»Ÿ');
            initializeVAD();
        }
    }, [localParticipant, initializeVAD]);

    // æ¸…ç†å‡½æ•°
    useEffect(() => {
        return () => {
            console.log('ğŸ§¹ æ¸…ç†VADåŒè½¨é“ç³»ç»Ÿ');
            if (isActive) {
                stopVAD();
            }
            if (vadProcessorRef.current) {
                vadProcessorRef.current.dispose();
                vadProcessorRef.current = null;
            }
            if (audioGatewayRef.current) {
                audioGatewayRef.current.dispose();
                audioGatewayRef.current = null;
            }
        };
    }, []);

    return {
        vadResult,
        isActive,
        startVAD,
        stopVAD,
        updateThreshold,
        vadProcessor: vadProcessorRef.current,
        isGatewayControlling,
        gatewayState
    };
}