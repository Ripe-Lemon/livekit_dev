'use client';

import { useState, useCallback, useEffect } from 'react';
import { Track } from 'livekit-client';

export interface TrackAnalysisResult {
    participantId: string;
    trackSid: string;
    trackName: string;
    source: string;
    kind: string;
    isMuted: boolean;
    isSubscribed: boolean;
    isEnabled: boolean;
    track?: any;
    mediaTrackInfo?: {
        label: string;
        id: string;
        readyState: string;
        enabled: boolean;
        muted: boolean;
        settings: any;
        constraints?: any;
        capabilities?: any;
    };
    vadAnalysis?: {
        æ˜¯å¦VADå¤„ç†: boolean;
        å›å£°æ¶ˆé™¤: boolean | undefined;
        å™ªå£°æŠ‘åˆ¶: boolean | undefined;
        è‡ªåŠ¨å¢ç›Š: boolean | undefined;
        é‡‡æ ·ç‡: number | undefined;
        å£°é“æ•°: number | undefined;
        è®¾å¤‡ID: string | undefined;
    };
    timestamp: string;
}

export interface VolumeMonitorData {
    participantId: string;
    timestamp: number;
    rmsVolume: number;
    peakVolume: number;
    trackState: string;
    trackEnabled: boolean;
    publicationMuted: boolean;
    subscriptionState: boolean;
}

export function useTrackDebugger(room: any) {
    const [trackAnalysis, setTrackAnalysis] = useState<TrackAnalysisResult[]>([]);
    const [volumeMonitors, setVolumeMonitors] = useState<Map<string, any>>(new Map());
    const [isAutoMonitoring, setIsAutoMonitoring] = useState(false);

    // åˆ†æå•ä¸ªéŸ³é¢‘è½¨é“
    const analyzeAudioTrack = useCallback((track: any, participant: any, publication?: any): TrackAnalysisResult | null => {
        if (!track || track.kind !== 'audio') return null;

        const mediaStreamTrack = track.mediaStreamTrack;
        if (!mediaStreamTrack) return null;

        const trackInfo: TrackAnalysisResult = {
            participantId: participant.identity,
            trackSid: publication?.trackSid || track.sid || track.id,
            trackName: publication?.trackName || track.name || 'unnamed',
            source: publication?.source || track.source || 'unknown',
            kind: track.kind,
            isMuted: publication?.isMuted || false,
            isSubscribed: publication?.isSubscribed || false,
            isEnabled: publication?.isEnabled || true,
            track: track,
            timestamp: new Date().toISOString()
        };

        // åª’ä½“è½¨é“ä¿¡æ¯
        trackInfo.mediaTrackInfo = {
            label: mediaStreamTrack.label,
            id: mediaStreamTrack.id,
            readyState: mediaStreamTrack.readyState,
            enabled: mediaStreamTrack.enabled,
            muted: mediaStreamTrack.muted,
            settings: mediaStreamTrack.getSettings()
        };

        // è·å–çº¦æŸä¿¡æ¯
        try {
            trackInfo.mediaTrackInfo.constraints = mediaStreamTrack.getConstraints();
        } catch (error) {
            console.warn('æ— æ³•è·å–éŸ³é¢‘çº¦æŸ:', error);
        }

        // è·å–èƒ½åŠ›ä¿¡æ¯
        try {
            trackInfo.mediaTrackInfo.capabilities = mediaStreamTrack.getCapabilities();
        } catch (error) {
            console.warn('æ— æ³•è·å–éŸ³é¢‘èƒ½åŠ›:', error);
        }

        // VADåˆ†æ
        if (trackInfo.mediaTrackInfo.settings) {
            const settings = trackInfo.mediaTrackInfo.settings;
            trackInfo.vadAnalysis = {
                æ˜¯å¦VADå¤„ç†: trackInfo.trackName === 'microphone' && 
                           (settings.echoCancellation === false ||
                            settings.noiseSuppression === false ||
                            settings.autoGainControl === false),
                å›å£°æ¶ˆé™¤: settings.echoCancellation,
                å™ªå£°æŠ‘åˆ¶: settings.noiseSuppression,
                è‡ªåŠ¨å¢ç›Š: settings.autoGainControl,
                é‡‡æ ·ç‡: settings.sampleRate,
                å£°é“æ•°: settings.channelCount,
                è®¾å¤‡ID: settings.deviceId
            };
        }

        return trackInfo;
    }, []);

    // è·å–æ‰€æœ‰è¿œç¨‹å‚ä¸è€…çš„éŸ³è½¨ä¿¡æ¯
    const analyzeAllRemoteTracks = useCallback(() => {
        if (!room) return [];

        const analysis: TrackAnalysisResult[] = [];
        
        console.group('ğŸ” åˆ†ææ‰€æœ‰è¿œç¨‹éŸ³è½¨');
        
        room.remoteParticipants.forEach((participant: any) => {
            console.group(`ğŸ‘¤ å‚ä¸è€…: ${participant.identity}`);
            
            participant.audioTrackPublications.forEach((publication: any) => {
                const trackInfo = analyzeAudioTrack(publication.track, participant, publication);
                
                if (trackInfo) {
                    analysis.push(trackInfo);
                    
                    console.log('ğŸ¤ éŸ³è½¨è¯¦ç»†ä¿¡æ¯:', {
                        åŸºç¡€ä¿¡æ¯: {
                            å‚ä¸è€…: trackInfo.participantId,
                            è½¨é“ID: trackInfo.trackSid,
                            è½¨é“åç§°: trackInfo.trackName,
                            éŸ³é¢‘æº: trackInfo.source,
                            æ˜¯å¦é™éŸ³: trackInfo.isMuted,
                            è®¢é˜…çŠ¶æ€: trackInfo.isSubscribed
                        },
                        è½¨é“è¯¦æƒ…: trackInfo.mediaTrackInfo,
                        VADåˆ†æ: trackInfo.vadAnalysis
                    });

                    // åˆ¤æ–­è½¨é“ç±»å‹
                    if (trackInfo.vadAnalysis?.æ˜¯å¦VADå¤„ç†) {
                        console.log('âœ… è¿™æ˜¯VADå¤„ç†åçš„éŸ³è½¨ï¼ˆéŸ³é¢‘å¤„ç†å·²å…³é—­ï¼‰');
                    } else {
                        console.log('ğŸ“¢ è¿™æ˜¯æ ‡å‡†éŸ³è½¨ï¼ˆå¯ç”¨äº†éŸ³é¢‘å¤„ç†ï¼‰');
                    }
                }
            });
            
            console.groupEnd();
        });

        console.groupEnd();
        
        setTrackAnalysis(analysis);
        return analysis;
    }, [room, analyzeAudioTrack]);

    // ç›‘æ§å•ä¸ªè¿œç¨‹éŸ³è½¨éŸ³é‡
    const monitorRemoteTrackVolume = useCallback((
        participantId: string, 
        duration: number = 10000,
        onVolumeUpdate?: (data: VolumeMonitorData) => void
    ) => {
        if (!room) {
            console.warn('æˆ¿é—´ä¸å­˜åœ¨');
            return null;
        }

        const participant = Array.from(room.remoteParticipants.values())
            .find((p: any) => p.identity === participantId) as { audioTrackPublications: Map<string, any>; identity: string };

        if (!participant) {
            console.warn(`æ‰¾ä¸åˆ°å‚ä¸è€…: ${participantId}`);
            return null;
        }

        const audioPublication = Array.from(participant.audioTrackPublications.values())[0];
        if (!audioPublication || !audioPublication.track) {
            console.warn(`å‚ä¸è€… ${participantId} æ²¡æœ‰éŸ³é¢‘è½¨é“`);
            return null;
        }

        const track = audioPublication.track.mediaStreamTrack;
        const stream = new MediaStream([track]);
        
        let audioContext: AudioContext;
        let source: MediaStreamAudioSourceNode;
        let analyser: AnalyserNode;
        
        try {
            audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
            source = audioContext.createMediaStreamSource(stream);
            analyser = audioContext.createAnalyser();
            
            analyser.fftSize = 2048;
            analyser.smoothingTimeConstant = 0.3;
            analyser.minDecibels = -100;
            analyser.maxDecibels = -10;
            
            source.connect(analyser);
        } catch (error) {
            console.error('åˆ›å»ºéŸ³é¢‘åˆ†æå™¨å¤±è´¥:', error);
            return null;
        }
        
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        
        console.log(`ğŸ§ å¼€å§‹ç›‘æ§ ${participantId} çš„éŸ³é¢‘è½¨é“ ${duration/1000} ç§’`);
        console.log('ğŸ” éŸ³é¢‘åˆ†æå™¨è®¾ç½®:', {
            fftSize: analyser.fftSize,
            frequencyBinCount: analyser.frequencyBinCount,
            minDecibels: analyser.minDecibels,
            maxDecibels: analyser.maxDecibels,
            smoothingTimeConstant: analyser.smoothingTimeConstant,
            audioContextSampleRate: audioContext.sampleRate,
            audioContextState: audioContext.state
        });
        
        let sampleCount = 0;
        const startTime = Date.now();
        
        const monitor = () => {
            analyser.getByteTimeDomainData(dataArray);
            
            // è®¡ç®—å¤šç§éŸ³é‡æŒ‡æ ‡
            let sum = 0;
            let peak = 0;
            let nonZeroCount = 0;
            
            for (let i = 0; i < dataArray.length; i++) {
                const amplitude = Math.abs(dataArray[i] - 128) / 128;
                sum += amplitude * amplitude;
                peak = Math.max(peak, amplitude);
                if (dataArray[i] !== 128) nonZeroCount++;
            }
            
            const rmsVolume = Math.sqrt(sum / dataArray.length);
            const dataPercent = (nonZeroCount / dataArray.length) * 100;
            
            sampleCount++;
            const elapsed = Date.now() - startTime;
            
            const volumeData: VolumeMonitorData = {
                participantId,
                timestamp: Date.now(),
                rmsVolume,
                peakVolume: peak,
                trackState: track.readyState,
                trackEnabled: track.enabled,
                publicationMuted: audioPublication.isMuted,
                subscriptionState: audioPublication.isSubscribed
            };
            
            // è°ƒç”¨å›è°ƒå‡½æ•°
            if (onVolumeUpdate) {
                onVolumeUpdate(volumeData);
            }
            
            if (rmsVolume > 0.005 || sampleCount % 5 === 0) {
                console.log(`ğŸµ ${participantId} éŸ³é‡ç›‘æ§ [${sampleCount}] - ${(elapsed/1000).toFixed(1)}s:`, {
                    RMSéŸ³é‡: rmsVolume.toFixed(6),
                    å³°å€¼éŸ³é‡: peak.toFixed(6),
                    æ•°æ®è¦†ç›–ç‡: `${dataPercent.toFixed(1)}%`,
                    è½¨é“çŠ¶æ€: track.readyState,
                    è½¨é“å¯ç”¨: track.enabled,
                    å‘å¸ƒé™éŸ³: audioPublication.isMuted,
                    è®¢é˜…çŠ¶æ€: audioPublication.isSubscribed,
                    éŸ³é¢‘ä¸Šä¸‹æ–‡çŠ¶æ€: audioContext.state,
                    æ ·æœ¬æ•°æ®: Array.from(dataArray.slice(0, 8))
                });
            }
        };
        
        const interval = setInterval(monitor, 1000);
        
        // å­˜å‚¨ç›‘æ§å™¨å¼•ç”¨
        const monitorRef = {
            interval,
            audioContext,
            participantId,
            startTime
        };
        
        volumeMonitors.set(participantId, monitorRef);
        setVolumeMonitors(new Map(volumeMonitors));
        
        // å®šæ—¶æ¸…ç†
        const timeout = setTimeout(() => {
            clearInterval(interval);
            audioContext.close();
            volumeMonitors.delete(participantId);
            setVolumeMonitors(new Map(volumeMonitors));
            console.log(`ğŸ”š åœæ­¢ç›‘æ§ ${participantId} çš„éŸ³é¢‘è½¨é“`);
        }, duration);
        
        return {
            stop: () => {
                clearInterval(interval);
                clearTimeout(timeout);
                audioContext.close();
                volumeMonitors.delete(participantId);
                setVolumeMonitors(new Map(volumeMonitors));
                console.log(`â¹ï¸ æ‰‹åŠ¨åœæ­¢ç›‘æ§ ${participantId} çš„éŸ³é¢‘è½¨é“`);
            }
        };
        
    }, [room, volumeMonitors]);

    // åœæ­¢æ‰€æœ‰éŸ³é‡ç›‘æ§
    const stopAllVolumeMonitors = useCallback(() => {
        volumeMonitors.forEach((monitor, participantId) => {
            clearInterval(monitor.interval);
            if (monitor.audioContext && monitor.audioContext.state !== 'closed') {
                monitor.audioContext.close();
            }
            console.log(`ğŸ›‘ åœæ­¢ç›‘æ§ ${participantId}`);
        });
        
        volumeMonitors.clear();
        setVolumeMonitors(new Map());
        setIsAutoMonitoring(false);
        
        console.log('ğŸ”š æ‰€æœ‰éŸ³é‡ç›‘æ§å·²åœæ­¢');
    }, [volumeMonitors]);

    // è‡ªåŠ¨ç›‘æ§æ‰€æœ‰æ´»è·ƒçš„è¿œç¨‹éŸ³è½¨
    const startAutoMonitoring = useCallback((duration: number = 30000) => {
        if (isAutoMonitoring) {
            console.warn('è‡ªåŠ¨ç›‘æ§å·²åœ¨è¿è¡Œä¸­');
            return;
        }

        console.log('ğŸš€ å¼€å§‹è‡ªåŠ¨ç›‘æ§æ‰€æœ‰è¿œç¨‹éŸ³è½¨...');
        setIsAutoMonitoring(true);
        
        const analysis = analyzeAllRemoteTracks();
        
        analysis.forEach((trackInfo, index) => {
            if (trackInfo.track && !trackInfo.isMuted && trackInfo.isSubscribed) {
                console.log(`ğŸ§ è‡ªåŠ¨å¼€å§‹ç›‘æ§: ${trackInfo.participantId}`);
                
                // é”™å¼€å¯åŠ¨æ—¶é—´é¿å…å†²çª
                setTimeout(() => {
                    monitorRemoteTrackVolume(trackInfo.participantId, duration);
                }, index * 500);
            }
        });
        
        // è‡ªåŠ¨åœæ­¢
        setTimeout(() => {
            setIsAutoMonitoring(false);
            console.log('ğŸ”š è‡ªåŠ¨ç›‘æ§æ—¶é—´ç»“æŸ');
        }, duration + 5000); // å¤šç­‰5ç§’ç¡®ä¿æ‰€æœ‰ç›‘æ§éƒ½ç»“æŸ
        
    }, [isAutoMonitoring, analyzeAllRemoteTracks, monitorRemoteTrackVolume]);

    // è·å–ç›‘æ§çŠ¶æ€
    const getMonitoringStatus = useCallback(() => {
        return {
            activeMonitors: Array.from(volumeMonitors.keys()),
            isAutoMonitoring,
            totalActiveMonitors: volumeMonitors.size
        };
    }, [volumeMonitors, isAutoMonitoring]);

    // æ¸…ç†å‡½æ•°
    useEffect(() => {
        return () => {
            stopAllVolumeMonitors();
        };
    }, [stopAllVolumeMonitors]);

    return {
        // çŠ¶æ€
        trackAnalysis,
        volumeMonitors,
        isAutoMonitoring,
        
        // åˆ†æåŠŸèƒ½
        analyzeAudioTrack,
        analyzeAllRemoteTracks,
        
        // ç›‘æ§åŠŸèƒ½
        monitorRemoteTrackVolume,
        stopAllVolumeMonitors,
        startAutoMonitoring,
        getMonitoringStatus
    };
}